{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "f1be1d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import openai\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "import pandas.io.formats.style\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import chromadb\n",
    "import tiktoken\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "import torch.nn.functional as F\n",
    "import sqlite3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e5ab8127",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_CHROMA = \"../../chroma_db\"\n",
    "client = chromadb.PersistentClient(path=PATH_CHROMA)\n",
    "collection = client.get_or_create_collection(name=\"interview_data\")\n",
    "embedding_model = \"Snowflake/snowflake-arctic-embed-l-v2.0\"\n",
    "sentence_model = SentenceTransformer(embedding_model).to(torch.device(\"cuda\"))\n",
    "load_dotenv()\n",
    "OPEN_AI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_key = OPEN_AI_API_KEY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73af9b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f519f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"append is a method used to add elements to the list in python. It adds element on the end of the list\"\n",
    "# query_wrong = \"append is a method which adds element at the front of the list\"\n",
    "\n",
    "# results = collection.query(\n",
    "#     query_embeddings=[sentence_model.encode(query, convert_to_numpy=True)],\n",
    "#     n_results=3\n",
    "# )\n",
    "\n",
    "# query = \"append is a method used to add elements to the list in python.\"\n",
    "# query_wrong = \"append which is used to print elements to the console\"\n",
    "\n",
    "# query_emb = sentence_model.encode(query, convert_to_tensor=True)\n",
    "# query_wrong_emb = sentence_model.encode(query_wrong, convert_to_tensor=True)\n",
    "# doc_embs = sentence_model.encode(results[\"documents\"][0], convert_to_tensor=True)\n",
    "\n",
    "# similarities = util.cos_sim(query_emb, doc_embs)\n",
    "# similarities_wrong = util.cos_sim(query_wrong_emb, doc_embs)\n",
    "\n",
    "# for i, (score, score_wrong) in enumerate(zip(similarities[0], similarities_wrong[0])):\n",
    "#     print(f\"Dokument {i+1}: Similarity = {score.item():.4f}, Similarity wrong = {score_wrong.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85bb666",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f75177",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a114a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaDatas(BaseModel):\n",
    "    difficulty: str\n",
    "    type_question: str\n",
    "    question: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "ac2df0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ideal_answer(difficulty, type_question, question, user_answer):\n",
    "    metadatas = MetaDatas(difficulty=difficulty, type_question=type_question, question=question)\n",
    "\n",
    "    results = collection.query(\n",
    "        query_embeddings=[sentence_model.encode(user_answer, convert_to_numpy=True)],\n",
    "        n_results=3,\n",
    "        where={\n",
    "            \"question\": metadatas.question\n",
    "        },\n",
    "        include=[\"embeddings\", \"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # answer_emb = sentence_model.encode(user_answer, convert_to_tensor=True).to(device)\n",
    "    # doc_embs_list = results[\"embeddings\"][0]\n",
    "    # doc_embs = torch.tensor(doc_embs_list, dtype=torch.float32, device=device)\n",
    "    # similarities = util.cos_sim(answer_emb, doc_embs)\n",
    "    # best_index = torch.argmax(similarities).item()\n",
    "\n",
    "    # ideal_answer = results[\"documents\"][0][best_index]\n",
    "\n",
    "\n",
    "    # return ideal_answer, similarities[0][best_index]\n",
    "\n",
    "    ideal_answer = results[\"documents\"][0][0]\n",
    "    return ideal_answer, torch.tensor([1.0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "fb84266b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_about_answer(ideal_answer, cosine, question, answer, result):\n",
    "    print(\"question: \" + question)\n",
    "    print(\"answer: \" + answer)\n",
    "    print(\"ideal_answer: \" + ideal_answer)\n",
    "    print(f\"cosine: {cosine.item()}\")\n",
    "    print(f\"grade: {result.grade}\")\n",
    "    print(\"explanation: \" + result.explanation_of_grade)\n",
    "    print(\"follow up: \" + result.follow_up_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "66ff12b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': [['LLM_QUESTION_4_1', 'LLM_QUESTION_4_3', 'LLM_QUESTION_4_2']], 'embeddings': [array([[-0.0022775 , -0.05408627,  0.01424893, ...,  0.00312287,\n",
      "        -0.01206894, -0.00482484],\n",
      "       [ 0.02553354, -0.04169761,  0.0512677 , ...,  0.00374069,\n",
      "        -0.01328347,  0.00933691],\n",
      "       [ 0.0151357 , -0.05960227,  0.04669798, ..., -0.00019255,\n",
      "        -0.02782513, -0.0013969 ]])], 'documents': [['It is a phenomenon where a model loses the ability to perform well on previously learned tasks after being fine-tuned on new tasks.', \"Catastrophic forgetting in fine-tuning large language models (LLMs) occurs when the model's performance on previous tasks degrades significantly after being trained on new tasks. This happens because the model's parameters adjust to optimize performance on the new task, potentially overwriting weights that were crucial for older tasks. It's a challenge in continual learning, as it requires balancing learning new information with retaining old knowledge.\", 'In fine-tuning LLMs, catastrophic forgetting is when the model forgets its pre-trained knowledge while adapting to new data.']], 'uris': None, 'included': ['embeddings', 'documents', 'metadatas', 'distances'], 'data': None, 'metadatas': [[{'question': 'What is catastrophic forgetting in fine-tuning LLMs?', 'difficulty': 'Medium', 'type_question': 'llm'}, {'type_question': 'llm', 'difficulty': 'Medium', 'question': 'What is catastrophic forgetting in fine-tuning LLMs?'}, {'type_question': 'llm', 'question': 'What is catastrophic forgetting in fine-tuning LLMs?', 'difficulty': 'Medium'}]], 'distances': [[1.0122839212417603, 1.1587997674942017, 1.187476396560669]]}\n"
     ]
    }
   ],
   "source": [
    "question = \"What is catastrophic forgetting in fine-tuning LLMs?\"\n",
    "user_answer = \"overfitting happens if model fits to well to training data\"\n",
    "ideal_answer, cosine_similarity = get_ideal_answer(\"Easy\", \"llm\", question, user_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a307ffe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is a phenomenon where a model loses the ability to perform well on previously learned tasks after being fine-tuned on new tasks. tensor(0.4939, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(ideal_answer, cosine_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "ba457379",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradedAnswer(BaseModel):\n",
    "    grade: int\n",
    "    explanation_of_grade: str\n",
    "    follow_up_question: str\n",
    "\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=GradedAnswer)\n",
    "\n",
    "format_instructions = parser.get_format_instructions().replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an interviewer who checks person's knowledge in llm.\"),\n",
    "    (\"human\", f\"\"\"\n",
    "        Evaluate user's answer on the question from 1 to 10, based on the user answer, ideal answer and cosine similarity calculated between both.\n",
    "        If user's answer does not contain whole information about ideal answer, provide follow-up question to suggest what is missing in the answer.\n",
    "        If user's answer contain whole information provide in follow_up_question field: \"DONE\"\n",
    "        Grade user better if his voice emotion is positive.\n",
    "     \n",
    "        Question: {{question}}\n",
    "        User answer: {{user_answer}}\n",
    "        Ideal answer: {{ideal_answer}}\n",
    "        User's emotion: {{emotion}}\n",
    "     \n",
    "        Return the result strictly in this JSON format:\n",
    "        {format_instructions}\n",
    "     \"\"\")\n",
    "])\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.1, api_key=OPEN_AI_API_KEY) \n",
    "\n",
    "chain = prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "51e17837",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Do you need a vector store for all text-based LLM use cases?\"\n",
    "user_answer = \"For chatbots or conversational agents that rely on predefined rule-based responses, a vector store might not be necessary. It is useful if we want our llm to provide answers based on some specific documents. As an alternative for vector databases we can use decision trees. It is very useful for retrieving information from a large text corpus because we can semantically find best answer for our prompts instead of using basic text NLP algorithms\"\n",
    "ideal_answer, cosine_similarity = get_ideal_answer(\"Easy\", \"llm\", question, user_answer)\n",
    "\n",
    "\n",
    "result = chain.invoke({\n",
    "    \"question\": question,\n",
    "    \"user_answer\": user_answer,\n",
    "    \"ideal_answer\": ideal_answer,\n",
    "    \"emotion\": \"happy\",\n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "5c319c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: Do you need a vector store for all text-based LLM use cases?\n",
      "answer: For chatbots or conversational agents that rely on predefined rule-based responses, a vector store might not be necessary. It is useful if we want our llm to provide answers based on some specific documents. As an alternative for vector databases we can use decision trees. It is very useful for retrieving information from a large text corpus because we can semantically find best answer for our prompts instead of using basic text NLP algorithms\n",
      "ideal_answer: For chatbots or conversational agents that rely on predefined rule-based responses, a vector store might not be necessary. Instead, these systems often use decision trees or intent recognition to guide responses. Nonetheless, if the chatbot needs to retrieve information from a large text corpus dynamically, integrating a vector store could improve its capabilities.\n",
      "cosine: 1.0\n",
      "grade: 8\n",
      "explanation: The user's answer contains most of the key points from the ideal answer, including the mention of decision trees and the context of when a vector store is useful. However, it lacks the explicit mention of improving capabilities when retrieving information dynamically from a large text corpus, which is a crucial part of the ideal answer.\n",
      "follow up: What do you think are the benefits of integrating a vector store for dynamic information retrieval?\n"
     ]
    }
   ],
   "source": [
    "get_data_about_answer(ideal_answer, cosine_similarity, question, user_answer, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382de7ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a266af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4dd411d",
   "metadata": {},
   "source": [
    "### 1. Start interview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9715370a",
   "metadata": {},
   "source": [
    "### 1.1 Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "9b74082c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StartInterviewModel(BaseModel):\n",
    "    introduction: str\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=StartInterviewModel)\n",
    "format_instructions = parser.get_format_instructions().replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an interviewer who checks person's knowledge in llm.\"),\n",
    "    (\"human\", f\"\"\"\n",
    "        Start interview by introducing yourself that you are interviewer in Interview Better company.\n",
    "     \n",
    "        Return the result strictly in this JSON format:\n",
    "        {format_instructions}\n",
    "     \"\"\")\n",
    "])\n",
    "\n",
    "start_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.1, api_key=OPEN_AI_API_KEY) \n",
    "\n",
    "chain = prompt | start_llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "592a7df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.invoke({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "a0c16246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "introduction='Hello, I am the interviewer from Interview Better company. I will be assessing your knowledge in large language models (LLMs) today.'\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911dabe6",
   "metadata": {},
   "source": [
    "### 1.2 Finding question to ask in interview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "7c6fac83",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DB = \"../../documents.db\"\n",
    "\n",
    "def get_random_questions_by_type(search_question_type, limit=10):\n",
    "    conn = sqlite3.connect(PATH_DB)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(f\"\"\"\n",
    "        SELECT question FROM documents\n",
    "        WHERE type_question LIKE ?\n",
    "        ORDER BY RANDOM()\n",
    "        LIMIT {limit};\n",
    "    \"\"\", (f\"%{search_question_type}%\", ))\n",
    "\n",
    "    results = cursor.fetchall()\n",
    "    conn.close()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "f0cd0c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = get_random_questions_by_type(\"llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "6f8c30e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('How do subword tokenization algorithms like Byte Pair Encoding (BPE) and WordPiece enhance LLMs?',), ('How can LLMs mitigate catastrophic forgetting during fine-tuning?',), ('How does Adaptive Softmax speed up large language models?',), ('Do you need a vector store for all text-based LLM use cases?',), ('What are the key steps involved in the Retrieval-Augmented Generation (RAG) pipeline?',), ('How does prompt engineering influence LLM performance, and what strategies can be used to optimize it?',), ('What is “reward hacking” in Reinforcement Learning from Human Feedback (RLHF)?',), ('What is the impact of scaling laws on the design of LLMs?',), ('How does the planner agent in AgenticRAG handle complex queries?',), ('How does contrastive learning improve LLM representations?',)]\n"
     ]
    }
   ],
   "source": [
    "print(questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3782c649",
   "metadata": {},
   "source": [
    "### 2. Continue Interview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4219abcd",
   "metadata": {},
   "source": [
    "### 3. Finalize Interview"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interview_better",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
