{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f1be1d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import openai\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "import pandas.io.formats.style\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import chromadb\n",
    "import tiktoken\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "import torch.nn.functional as F\n",
    "import sqlite3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e5ab8127",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_CHROMA = \"../../chroma_db\"\n",
    "client = chromadb.PersistentClient(path=PATH_CHROMA)\n",
    "collection = client.get_or_create_collection(name=\"interview_data\")\n",
    "embedding_model = \"Snowflake/snowflake-arctic-embed-l-v2.0\"\n",
    "sentence_model = SentenceTransformer(embedding_model).to(torch.device(\"cuda\"))\n",
    "load_dotenv()\n",
    "OPEN_AI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_key = OPEN_AI_API_KEY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73af9b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f519f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"append is a method used to add elements to the list in python. It adds element on the end of the list\"\n",
    "# query_wrong = \"append is a method which adds element at the front of the list\"\n",
    "\n",
    "# results = collection.query(\n",
    "#     query_embeddings=[sentence_model.encode(query, convert_to_numpy=True)],\n",
    "#     n_results=3\n",
    "# )\n",
    "\n",
    "# query = \"append is a method used to add elements to the list in python.\"\n",
    "# query_wrong = \"append which is used to print elements to the console\"\n",
    "\n",
    "# query_emb = sentence_model.encode(query, convert_to_tensor=True)\n",
    "# query_wrong_emb = sentence_model.encode(query_wrong, convert_to_tensor=True)\n",
    "# doc_embs = sentence_model.encode(results[\"documents\"][0], convert_to_tensor=True)\n",
    "\n",
    "# similarities = util.cos_sim(query_emb, doc_embs)\n",
    "# similarities_wrong = util.cos_sim(query_wrong_emb, doc_embs)\n",
    "\n",
    "# for i, (score, score_wrong) in enumerate(zip(similarities[0], similarities_wrong[0])):\n",
    "#     print(f\"Dokument {i+1}: Similarity = {score.item():.4f}, Similarity wrong = {score_wrong.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85bb666",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f75177",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "5a114a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaDatas(BaseModel):\n",
    "    difficulty: str\n",
    "    type_question: str\n",
    "    question: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "ac2df0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ideal_answer(difficulty, type_question, question, user_answer):\n",
    "    metadatas = MetaDatas(difficulty=difficulty, type_question=type_question, question=question)\n",
    "\n",
    "    results = collection.query(\n",
    "        query_embeddings=[sentence_model.encode(user_answer, convert_to_numpy=True)],\n",
    "        n_results=3,\n",
    "        where={\n",
    "            \"question\": metadatas.question\n",
    "        },\n",
    "        include=[\"embeddings\", \"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # answer_emb = sentence_model.encode(user_answer, convert_to_tensor=True).to(device)\n",
    "    # doc_embs_list = results[\"embeddings\"][0]\n",
    "    # doc_embs = torch.tensor(doc_embs_list, dtype=torch.float32, device=device)\n",
    "    # similarities = util.cos_sim(answer_emb, doc_embs)\n",
    "    # best_index = torch.argmax(similarities).item()\n",
    "\n",
    "    # ideal_answer = results[\"documents\"][0][best_index]\n",
    "\n",
    "\n",
    "    # return ideal_answer, similarities[0][best_index]\n",
    "\n",
    "    ideal_answer = results[\"documents\"][0][0]\n",
    "    return ideal_answer, torch.tensor([1.0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "fb84266b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_about_answer(ideal_answer, cosine, question, answer, result):\n",
    "    print(\"question: \" + question)\n",
    "    print(\"answer: \" + answer)\n",
    "    print(\"ideal_answer: \" + ideal_answer)\n",
    "    print(f\"cosine: {cosine.item()}\")\n",
    "    print(f\"grade: {result.grade}\")\n",
    "    print(\"explanation: \" + result.explanation_of_grade)\n",
    "    print(\"follow up: \" + result.follow_up_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "66ff12b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is catastrophic forgetting in fine-tuning LLMs?\"\n",
    "user_answer = \"overfitting happens if model fits to well to training data\"\n",
    "ideal_answer, cosine_similarity = get_ideal_answer(\"Easy\", \"llm\", question, user_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "a307ffe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is a phenomenon where a model loses the ability to perform well on previously learned tasks after being fine-tuned on new tasks. tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "print(ideal_answer, cosine_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba457379",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e17837",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c319c30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382de7ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "55a266af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MessageHistory(BaseModel):\n",
    "    role: str\n",
    "    content: str\n",
    "\n",
    "class RedisMock:\n",
    "    def __init__(self):\n",
    "        self.messages = []\n",
    "\n",
    "    def add_message(self, role, content):\n",
    "        message = MessageHistory(role=role, content=content)\n",
    "        self.messages.append(message)\n",
    "\n",
    "    def get_recent_messages(self, limit=5):\n",
    "        return self.messages[-limit:]\n",
    "\n",
    "    def __str__(self):\n",
    "        history = \"\"\n",
    "        for message in self.messages:\n",
    "            history += f\"{message.role}: {message.content} \\n\"\n",
    "\n",
    "        return history\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "8785ae88",
   "metadata": {},
   "outputs": [],
   "source": [
    "redisMock = RedisMock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "b728e32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_with_instruction(history_obj: RedisMock, instruction: str):\n",
    "    recent_messages = history_obj.get_recent_messages(limit=10)\n",
    "\n",
    "    chat_messages = []\n",
    "    chat_messages.append((\"system\", \"You are an interviewer who checks person's knowledge in llm.\"))\n",
    "    \n",
    "    for msg in recent_messages:\n",
    "        chat_messages.append((msg.role, msg.content))\n",
    "\n",
    "    chat_messages.append((\"system\", instruction))\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages(chat_messages)\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dd411d",
   "metadata": {},
   "source": [
    "### 1. Start interview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9715370a",
   "metadata": {},
   "source": [
    "### 1.1 Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "2ec47c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StartInterviewModel(BaseModel):\n",
    "    introduction: str\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=StartInterviewModel)\n",
    "format_instructions_start_interview = parser.get_format_instructions().replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "\n",
    "START_INTERVIEW_INSTRUCTION = f\"Return the result strictly in this JSON format: \\n{format_instructions_start_interview}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "9b74082c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt_human = f\"\"\"\n",
    "        Start interview by introducing yourself that you are interviewer in Interview Better company.\n",
    "     \"\"\"\n",
    "\n",
    "redisMock.add_message(\"user\", prompt_human)\n",
    "prompt = build_prompt_with_instruction(redisMock, START_INTERVIEW_INSTRUCTION)\n",
    "start_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.1, api_key=OPEN_AI_API_KEY) \n",
    "chain = prompt | start_llm | parser\n",
    "result = chain.invoke({})\n",
    "\n",
    "\n",
    "redisMock.add_message(\"assistant\", result.introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "592a7df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: \n",
      "        Start interview by introducing yourself that you are interviewer in Interview Better company.\n",
      "      \n",
      "assistant: Hello, I am the interviewer from Interview Better company. Today, I will be assessing your knowledge in large language models. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(redisMock)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911dabe6",
   "metadata": {},
   "source": [
    "### 1.2 Finding question to ask in interview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "7c6fac83",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DB = \"../../documents.db\"\n",
    "\n",
    "def get_random_questions_by_type(search_question_type, limit=10):\n",
    "    conn = sqlite3.connect(PATH_DB)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(f\"\"\"\n",
    "        SELECT question FROM documents\n",
    "        WHERE type_question LIKE ?\n",
    "        ORDER BY RANDOM()\n",
    "        LIMIT {limit};\n",
    "    \"\"\", (f\"%{search_question_type}%\", ))\n",
    "\n",
    "    results = cursor.fetchall()\n",
    "    conn.close()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "f0cd0c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = get_random_questions_by_type(\"llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "6f8c30e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('How does contrastive learning improve LLM representations?',), ('How does task-specific fine-tuning differ from zero-shot learning in LLMs?',), ('How does Adaptive Softmax speed up large language models?',), ('How does Mixture of Experts (MoE) improve the efficiency of LLMs?',), ('What is Chain-of-Thought (CoT) prompting, and how does it improve complex reasoning in LLMs?',), ('How does knowledge graph integration enhance LLMs?',), ('What is the purpose of positional encoding in Transformer models?',), ('What is the role of contextual embeddings in LLMs, and how do they differ from static embeddings?',), ('How does multi-task learning benefit LLMs?',), ('How does scaling law analysis help optimize the design of LLMs?',)]\n"
     ]
    }
   ],
   "source": [
    "print(questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3782c649",
   "metadata": {},
   "source": [
    "### 2. Continue Interview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "5bba1dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradedAnswer(BaseModel):\n",
    "    grade: int\n",
    "    explanation_of_grade: str\n",
    "    follow_up_question: str\n",
    "\n",
    "\n",
    "parser_graded = PydanticOutputParser(pydantic_object=GradedAnswer)\n",
    "format_instructions_graded = parser.get_format_instructions().replace(\"{\", \"{{\").replace(\"}\", \"}}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "8f9074f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_INSTRUCTION = f\"\"\"\n",
    "Return the result strictly in this JSON format:\n",
    "\n",
    "{format_instructions_graded}\n",
    "\n",
    "Make sure the JSON object includes exactly these fields:\n",
    "- \"grade\" (integer)\n",
    "- \"explanation_of_grade\" (string)\n",
    "- \"follow_up_question\" (string)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "cc381470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: \n",
      "        Start interview by introducing yourself that you are interviewer in Interview Better company.\n",
      "      \n",
      "assistant: Hello, I am the interviewer from Interview Better company. Today, I will be assessing your knowledge in large language models. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(redisMock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "f6e30c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_grade_information(graded_result: GradedAnswer):\n",
    "    return f\"\"\"\n",
    "        grade: {graded_result.grade}\n",
    "        explanation of this grade: {graded_result.explanation_of_grade}\n",
    "        follow up question: {graded_result.follow_up_question}\n",
    "    \"\"\"\n",
    "\n",
    "def conversate_llm(question, user_answer, ideal_answer, emotion):\n",
    "    prompt_human = f\"\"\"\n",
    "        Evaluate user's answer on the question from 1 to 10, based on the user answer, ideal answer and cosine similarity calculated between both.\n",
    "        If user's answer does not contain whole information about ideal answer, provide follow-up question to suggest what is missing in the answer.\n",
    "        If user's answer contain whole information provide in follow_up_question field: \"DONE\"\n",
    "        Grade user better if his voice emotion is positive.\n",
    "        Remember to keep the JSON format.\n",
    "    \n",
    "        Question: {question}\n",
    "        User answer: {user_answer}\n",
    "        Ideal answer: {ideal_answer}\n",
    "        User's emotion: {emotion}\n",
    "    \"\"\"\n",
    "\n",
    "    redisMock.add_message(\"user\", prompt_human)\n",
    "    prompt = build_prompt_with_instruction(redisMock, FINAL_INSTRUCTION)\n",
    "    print(prompt.messages)\n",
    "\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.1, api_key=OPEN_AI_API_KEY)\n",
    "    chain = prompt | llm | parser_graded\n",
    "    graded_result = chain.invoke({})\n",
    "\n",
    "    redisMock.add_message(\"assistant\", add_grade_information(graded_result))\n",
    "\n",
    "    return graded_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "e5f42111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=\"You are an interviewer who checks person's knowledge in llm.\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='\\n        Start interview by introducing yourself that you are interviewer in Interview Better company.\\n     '), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Hello, I am the interviewer from Interview Better company. Today, I will be assessing your knowledge in large language models.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='\\n        Evaluate user\\'s answer on the question from 1 to 10, based on the user answer, ideal answer and cosine similarity calculated between both.\\n        If user\\'s answer does not contain whole information about ideal answer, provide follow-up question to suggest what is missing in the answer.\\n        If user\\'s answer contain whole information provide in follow_up_question field: \"DONE\"\\n        Grade user better if his voice emotion is positive.\\n        Remember to keep the JSON format.\\n    \\n        Question: Do you need a vector store for all text-based LLM use cases?\\n        User answer: For chatbots or conversational agents that rely on predefined rule-based responses, a vector store might not be necessary. It is useful if we want our llm to provide answers based on some specific documents. As an alternative for vector databases we can use decision trees. It is very useful for retrieving information from a large text corpus because we can semantically find best answer for our prompts instead of using basic text NLP algorithms\\n        Ideal answer: For chatbots or conversational agents that rely on predefined rule-based responses, a vector store might not be necessary. Instead, these systems often use decision trees or intent recognition to guide responses. Nonetheless, if the chatbot needs to retrieve information from a large text corpus dynamically, integrating a vector store could improve its capabilities.\\n        User\\'s emotion: happy\\n    '), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=\"\\n        grade: 8\\n        explanation of this grade: The user's answer contains most of the key points from the ideal answer, but it lacks the explicit mention of the improvement in capabilities when integrating a vector store for dynamic information retrieval. The positive emotion in the user's voice also contributed to a higher grade.\\n        follow up question: What specific improvements can a vector store provide for dynamic information retrieval in chatbots?\\n    \"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='\\n        Evaluate user\\'s answer on the question from 1 to 10, based on the user answer, ideal answer and cosine similarity calculated between both.\\n        If user\\'s answer does not contain whole information about ideal answer, provide follow-up question to suggest what is missing in the answer.\\n        If user\\'s answer contain whole information provide in follow_up_question field: \"DONE\"\\n        Grade user better if his voice emotion is positive.\\n        Remember to keep the JSON format.\\n    \\n        Question: Do you need a vector store for all text-based LLM use cases?\\n        User answer: For chatbots or conversational agents that rely on predefined rule-based responses, a vector store might not be necessary. It is useful if we want our llm to provide answers based on some specific documents. As an alternative for vector databases we can use decision trees. It is very useful for retrieving information from a large text corpus because we can semantically find best answer for our prompts instead of using basic text NLP algorithms\\n        Ideal answer: For chatbots or conversational agents that rely on predefined rule-based responses, a vector store might not be necessary. Instead, these systems often use decision trees or intent recognition to guide responses. Nonetheless, if the chatbot needs to retrieve information from a large text corpus dynamically, integrating a vector store could improve its capabilities.\\n        User\\'s emotion: happy\\n    '), additional_kwargs={}), SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='\\nReturn the result strictly in this JSON format:\\n\\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {{\"properties\": {{\"foo\": {{\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {{\"type\": \"string\"}}}}}}, \"required\": [\"foo\"]}}\\nthe object {{\"foo\": [\"bar\", \"baz\"]}} is a well-formatted instance of the schema. The object {{\"properties\": {{\"foo\": [\"bar\", \"baz\"]}}}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{{\"properties\": {{\"introduction\": {{\"title\": \"Introduction\", \"type\": \"string\"}}}}, \"required\": [\"introduction\"]}}\\n```\\n\\nMake sure the JSON object includes exactly these fields:\\n- \"grade\" (integer)\\n- \"explanation_of_grade\" (string)\\n- \"follow_up_question\" (string)'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "question = \"Do you need a vector store for all text-based LLM use cases?\"\n",
    "user_answer = \"For chatbots or conversational agents that rely on predefined rule-based responses, a vector store might not be necessary. It is useful if we want our llm to provide answers based on some specific documents. As an alternative for vector databases we can use decision trees. It is very useful for retrieving information from a large text corpus because we can semantically find best answer for our prompts instead of using basic text NLP algorithms\"\n",
    "ideal_answer, cosine_similarity = get_ideal_answer(\"Easy\", \"llm\", question, user_answer)\n",
    "emotion = \"happy\"\n",
    "\n",
    "graded_result = conversate_llm(question, user_answer, ideal_answer, emotion)\n",
    "\n",
    "# result = chain.invoke({\n",
    "#     \"question\": question,\n",
    "#     \"user_answer\": user_answer,\n",
    "#     \"ideal_answer\": ideal_answer,\n",
    "#     \"emotion\": \"happy\",\n",
    "    \n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "d3e3612f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: Do you need a vector store for all text-based LLM use cases?\n",
      "answer: For chatbots or conversational agents that rely on predefined rule-based responses, a vector store might not be necessary. It is useful if we want our llm to provide answers based on some specific documents. As an alternative for vector databases we can use decision trees. It is very useful for retrieving information from a large text corpus because we can semantically find best answer for our prompts instead of using basic text NLP algorithms\n",
      "ideal_answer: For chatbots or conversational agents that rely on predefined rule-based responses, a vector store might not be necessary. Instead, these systems often use decision trees or intent recognition to guide responses. Nonetheless, if the chatbot needs to retrieve information from a large text corpus dynamically, integrating a vector store could improve its capabilities.\n",
      "cosine: 1.0\n",
      "grade: 8\n",
      "explanation: The user's answer contains most of the key points from the ideal answer, but it lacks the explicit mention of the improvement in capabilities when integrating a vector store for dynamic information retrieval. The positive emotion in the user's voice also contributed to a higher grade.\n",
      "follow up: What specific improvements can a vector store provide for dynamic information retrieval in chatbots?\n"
     ]
    }
   ],
   "source": [
    "get_data_about_answer(ideal_answer, cosine_similarity, question, user_answer, graded_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "5a68655b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: \n",
      "        Start interview by introducing yourself that you are interviewer in Interview Better company.\n",
      "      \n",
      "assistant: Hello, I am the interviewer from Interview Better company. Today, I will be assessing your knowledge in large language models. \n",
      "user: \n",
      "        Evaluate user's answer on the question from 1 to 10, based on the user answer, ideal answer and cosine similarity calculated between both.\n",
      "        If user's answer does not contain whole information about ideal answer, provide follow-up question to suggest what is missing in the answer.\n",
      "        If user's answer contain whole information provide in follow_up_question field: \"DONE\"\n",
      "        Grade user better if his voice emotion is positive.\n",
      "        Remember to keep the JSON format.\n",
      "    \n",
      "        Question: Do you need a vector store for all text-based LLM use cases?\n",
      "        User answer: For chatbots or conversational agents that rely on predefined rule-based responses, a vector store might not be necessary. It is useful if we want our llm to provide answers based on some specific documents. As an alternative for vector databases we can use decision trees. It is very useful for retrieving information from a large text corpus because we can semantically find best answer for our prompts instead of using basic text NLP algorithms\n",
      "        Ideal answer: For chatbots or conversational agents that rely on predefined rule-based responses, a vector store might not be necessary. Instead, these systems often use decision trees or intent recognition to guide responses. Nonetheless, if the chatbot needs to retrieve information from a large text corpus dynamically, integrating a vector store could improve its capabilities.\n",
      "        User's emotion: happy\n",
      "     \n",
      "assistant: \n",
      "        grade: 8\n",
      "        explanation of this grade: The user's answer contains most of the key points from the ideal answer, but it lacks the explicit mention of the improvement in capabilities when integrating a vector store for dynamic information retrieval. The positive emotion in the user's voice also contributed to a higher grade.\n",
      "        follow up question: What specific improvements can a vector store provide for dynamic information retrieval in chatbots?\n",
      "     \n",
      "user: \n",
      "        Evaluate user's answer on the question from 1 to 10, based on the user answer, ideal answer and cosine similarity calculated between both.\n",
      "        If user's answer does not contain whole information about ideal answer, provide follow-up question to suggest what is missing in the answer.\n",
      "        If user's answer contain whole information provide in follow_up_question field: \"DONE\"\n",
      "        Grade user better if his voice emotion is positive.\n",
      "        Remember to keep the JSON format.\n",
      "    \n",
      "        Question: Do you need a vector store for all text-based LLM use cases?\n",
      "        User answer: For chatbots or conversational agents that rely on predefined rule-based responses, a vector store might not be necessary. It is useful if we want our llm to provide answers based on some specific documents. As an alternative for vector databases we can use decision trees. It is very useful for retrieving information from a large text corpus because we can semantically find best answer for our prompts instead of using basic text NLP algorithms\n",
      "        Ideal answer: For chatbots or conversational agents that rely on predefined rule-based responses, a vector store might not be necessary. Instead, these systems often use decision trees or intent recognition to guide responses. Nonetheless, if the chatbot needs to retrieve information from a large text corpus dynamically, integrating a vector store could improve its capabilities.\n",
      "        User's emotion: happy\n",
      "     \n",
      "assistant: \n",
      "        grade: 8\n",
      "        explanation of this grade: The user's answer contains most of the key points from the ideal answer, but it lacks the explicit mention of the improvement in capabilities when integrating a vector store for dynamic information retrieval. The positive emotion in the user's voice also contributed to a higher grade.\n",
      "        follow up question: What specific improvements can a vector store provide for dynamic information retrieval in chatbots?\n",
      "     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(redisMock)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4219abcd",
   "metadata": {},
   "source": [
    "### 3. Finalize Interview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add2ac24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interview_better",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
