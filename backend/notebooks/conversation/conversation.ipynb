{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1be1d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\kacpe\\anaconda3\\envs\\interview_better\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import openai\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "import pandas.io.formats.style\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import chromadb\n",
    "import tiktoken\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "import torch.nn.functional as F\n",
    "import sqlite3\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5ab8127",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_CHROMA = \"../../chroma_db\"\n",
    "client = chromadb.PersistentClient(path=PATH_CHROMA)\n",
    "collection = client.get_or_create_collection(name=\"interview_data\")\n",
    "embedding_model = \"Snowflake/snowflake-arctic-embed-l-v2.0\"\n",
    "sentence_model = SentenceTransformer(embedding_model).to(torch.device(\"cuda\"))\n",
    "load_dotenv()\n",
    "OPEN_AI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_key = OPEN_AI_API_KEY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73af9b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f519f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"append is a method used to add elements to the list in python. It adds element on the end of the list\"\n",
    "# query_wrong = \"append is a method which adds element at the front of the list\"\n",
    "\n",
    "# results = collection.query(\n",
    "#     query_embeddings=[sentence_model.encode(query, convert_to_numpy=True)],\n",
    "#     n_results=3\n",
    "# )\n",
    "\n",
    "# query = \"append is a method used to add elements to the list in python.\"\n",
    "# query_wrong = \"append which is used to print elements to the console\"\n",
    "\n",
    "# query_emb = sentence_model.encode(query, convert_to_tensor=True)\n",
    "# query_wrong_emb = sentence_model.encode(query_wrong, convert_to_tensor=True)\n",
    "# doc_embs = sentence_model.encode(results[\"documents\"][0], convert_to_tensor=True)\n",
    "\n",
    "# similarities = util.cos_sim(query_emb, doc_embs)\n",
    "# similarities_wrong = util.cos_sim(query_wrong_emb, doc_embs)\n",
    "\n",
    "# for i, (score, score_wrong) in enumerate(zip(similarities[0], similarities_wrong[0])):\n",
    "#     print(f\"Dokument {i+1}: Similarity = {score.item():.4f}, Similarity wrong = {score_wrong.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85bb666",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f75177",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a114a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaDatas(BaseModel):\n",
    "    difficulty: str\n",
    "    type_question: str\n",
    "    question: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac2df0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ideal_answer(difficulty, type_question, question, user_answer):\n",
    "    metadatas = MetaDatas(difficulty=difficulty, type_question=type_question, question=question)\n",
    "\n",
    "    results = collection.query(\n",
    "        query_embeddings=[sentence_model.encode(user_answer, convert_to_numpy=True)],\n",
    "        n_results=3,\n",
    "        where={\n",
    "            \"question\": metadatas.question\n",
    "        },\n",
    "        include=[\"embeddings\", \"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # answer_emb = sentence_model.encode(user_answer, convert_to_tensor=True).to(device)\n",
    "    # doc_embs_list = results[\"embeddings\"][0]\n",
    "    # doc_embs = torch.tensor(doc_embs_list, dtype=torch.float32, device=device)\n",
    "    # similarities = util.cos_sim(answer_emb, doc_embs)\n",
    "    # best_index = torch.argmax(similarities).item()\n",
    "\n",
    "    # ideal_answer = results[\"documents\"][0][best_index]\n",
    "\n",
    "\n",
    "    # return ideal_answer, similarities[0][best_index]\n",
    "\n",
    "    ideal_answer = results[\"documents\"][0][0]\n",
    "    return ideal_answer, torch.tensor([1.0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb84266b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_about_answer(ideal_answer, cosine, question, answer, result):\n",
    "    print(\"question: \" + question)\n",
    "    print(\"answer: \" + answer)\n",
    "    print(\"ideal_answer: \" + ideal_answer)\n",
    "    print(f\"cosine: {cosine.item()}\")\n",
    "    print(f\"grade: {result.grade}\")\n",
    "    print(\"explanation: \" + result.explanation_of_grade)\n",
    "    print(\"follow up: \" + result.follow_up_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66ff12b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is catastrophic forgetting in fine-tuning LLMs?\"\n",
    "user_answer = \"overfitting happens if model fits to well to training data\"\n",
    "ideal_answer, cosine_similarity = get_ideal_answer(\"Easy\", \"llm\", question, user_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a307ffe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is a phenomenon where a model loses the ability to perform well on previously learned tasks after being fine-tuned on new tasks. tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "print(ideal_answer, cosine_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba457379",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e17837",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c319c30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "382de7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_MESSAGE = \"You are an interviewer who checks person's knowledge in LLM.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55a266af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MessageHistory(BaseModel):\n",
    "    role: str\n",
    "    content: str\n",
    "\n",
    "class RedisMock:\n",
    "    def __init__(self):\n",
    "        self.messages = []\n",
    "        self.introduction_message = []\n",
    "\n",
    "    def add_message(self, role, content):\n",
    "        message = MessageHistory(role=role, content=content)\n",
    "        self.messages.append(message)\n",
    "\n",
    "    def add_introduction_message(self, role, content):\n",
    "        message = MessageHistory(role=role, content=content)\n",
    "        self.introduction_message.append(message)\n",
    "\n",
    "    def get_all_messages(self):\n",
    "        return self.introduction_message + self.messages\n",
    "    \n",
    "    def get_history(self):\n",
    "        return \"\\n\".join(f\"{m.role}: {m.content}\" for m in self.get_all_messages())\n",
    "\n",
    "    def get_recent_messages(self, limit=5):\n",
    "        return self.get_all_messages()[-limit:]\n",
    "\n",
    "    def print_last(self):\n",
    "        print(f\"{self.get_all_messages()[-2].role}: {self.get_all_messages()[-2].content}\")\n",
    "        print(f\"{self.get_all_messages()[-1].role}: {self.get_all_messages()[-1].content}\")\n",
    "\n",
    "    def clear_messages(self):\n",
    "        self.messages = []\n",
    "\n",
    "    def __str__(self):\n",
    "        history = \"\"\n",
    "        for message in self.get_all_messages():\n",
    "            history += f\"{message.role}: {message.content} \\n\"\n",
    "\n",
    "        return history\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8785ae88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b728e32f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4dd411d",
   "metadata": {},
   "source": [
    "### 1. Start interview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9715370a",
   "metadata": {},
   "source": [
    "### 1.1 Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ec47c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_with_instruction(\n",
    "        history_obj: RedisMock,\n",
    "        current_user_prompt: str,\n",
    "        format_instruction: str,\n",
    "        limit: int = 10,\n",
    ") -> ChatPromptTemplate:\n",
    "    chat: list[tuple[str, str]] = [(\"system\", SYSTEM_MESSAGE)]\n",
    "\n",
    "    recent = history_obj.get_recent_messages(limit=limit)\n",
    "    chat.extend((m.role, m.content) for m in recent)\n",
    "\n",
    "    chat.append((\"user\", current_user_prompt))\n",
    "    chat.append((\"system\", format_instruction))\n",
    "\n",
    "    return ChatPromptTemplate.from_messages(chat)\n",
    "\n",
    "class StartInterviewModel(BaseModel):\n",
    "    introduction: str\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=StartInterviewModel)\n",
    "format_instructions_start_interview = parser.get_format_instructions().replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "\n",
    "redisMock = RedisMock()\n",
    "\n",
    "START_INTERVIEW_INSTRUCTION = f\"Return the result strictly in this JSON format: \\n{format_instructions_start_interview}\"\n",
    "\n",
    "prompt_human = f\"\"\"\n",
    "        Start interview by introducing yourself that you are interviewer in Interview Better company.\n",
    "     \"\"\"\n",
    "\n",
    "redisMock.add_introduction_message(\"user\", prompt_human)\n",
    "prompt = build_prompt_with_instruction(redisMock, prompt_human, START_INTERVIEW_INSTRUCTION)\n",
    "start_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.1, api_key=OPEN_AI_API_KEY) \n",
    "chain = prompt | start_llm | parser\n",
    "result = chain.invoke({})\n",
    "\n",
    "\n",
    "redisMock.add_introduction_message(\"assistant\", result.introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "9b74082c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# prompt_human = f\"\"\"\n",
    "#         Start interview by introducing yourself that you are interviewer in Interview Better company.\n",
    "#      \"\"\"\n",
    "\n",
    "# redisMock.add_introduction_message(\"user\", prompt_human)\n",
    "# prompt = build_prompt_with_instruction(redisMock, prompt_human, START_INTERVIEW_INSTRUCTION)\n",
    "# start_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.1, api_key=OPEN_AI_API_KEY) \n",
    "# chain = prompt | start_llm | parser\n",
    "# result = chain.invoke({})\n",
    "\n",
    "\n",
    "# redisMock.add_introduction_message(\"assistant\", result.introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "592a7df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: \n",
      "        Start interview by introducing yourself that you are interviewer in Interview Better company.\n",
      "      \n",
      "assistant: Hello, my name is [Your Name], and I am an interviewer at Interview Better company. Today, I will be assessing your knowledge in the field of Large Language Models (LLMs). \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(redisMock)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911dabe6",
   "metadata": {},
   "source": [
    "### 1.2 Finding question to ask in interview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c6fac83",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DB = \"../../documents.db\"\n",
    "\n",
    "def get_random_questions_by_type(search_question_type, limit=10):\n",
    "    conn = sqlite3.connect(PATH_DB)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(f\"\"\"\n",
    "        SELECT question FROM documents\n",
    "        WHERE type_question LIKE ?\n",
    "        ORDER BY RANDOM()\n",
    "        LIMIT {limit};\n",
    "    \"\"\", (f\"%{search_question_type}%\", ))\n",
    "\n",
    "    results = cursor.fetchall()\n",
    "    conn.close()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0cd0c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [question[0] for question in get_random_questions_by_type(\"llm\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8c30e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_random_questions_by_type(\"llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "596e4369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['How does Chain-of-Thought (CoT) prompting improve reasoning in LLMs?', 'How can LLMs mitigate catastrophic forgetting during fine-tuning?', 'How does gradient checkpointing reduce memory usage in training large LLMs?', 'How does prompt engineering influence LLM performance, and what strategies can be used to optimize it?', 'What are the key steps involved in the Retrieval-Augmented Generation (RAG) pipeline?', 'What is the role of contextual embeddings in LLMs, and how do they differ from static embeddings?', 'How does beam search improve upon greedy decoding in LLMs?', 'What is catastrophic forgetting in fine-tuning LLMs?', 'How does meta-learning benefit LLMs?', 'What is knowledge distillation, and how is it used in LLMs?']\n"
     ]
    }
   ],
   "source": [
    "print(questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3782c649",
   "metadata": {},
   "source": [
    "### 2. Continue Interview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bba1dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradedAnswer(BaseModel):\n",
    "    grade: int = 0\n",
    "    explanation_of_grade: str = \"\"\n",
    "    follow_up_question: str = \"\"\n",
    "\n",
    "\n",
    "parser_graded = PydanticOutputParser(pydantic_object=GradedAnswer)\n",
    "format_instructions_graded = parser_graded.get_format_instructions().replace(\"{\", \"{{\").replace(\"}\", \"}}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f9074f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_INSTRUCTION = f\"\"\"\n",
    "Return the result strictly in this JSON format:\n",
    "\n",
    "{format_instructions_graded}\n",
    "\n",
    "Make sure the JSON object includes exactly these fields:\n",
    "- \"grade\" (integer)\n",
    "- \"explanation_of_grade\" (string)\n",
    "- \"follow_up_question\" (string)\"\"\".strip()\n",
    "\n",
    "EVALUATION_INSTRUCTION = \"\"\"\n",
    "Evaluate user's answer on the question from 1 to 10, based on the user answer, ideal answer and cosine similarity calculated between both.\n",
    "If user's answer does not contain whole information about ideal answer, provide follow-up question to suggest what is missing in the answer.\n",
    "If user's answer contain whole information provide in follow_up_question field: \\\"DONE\\\".\n",
    "If user answers that he doesn't know, also put \\\"DONE\\\".\n",
    "Grade user better if his voice emotion is positive.\n",
    "Remember to keep the JSON format.\n",
    "\"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc381470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: \n",
      "        Start interview by introducing yourself that you are interviewer in Interview Better company.\n",
      "      \n",
      "assistant: Hello, my name is [Your Name], and I am an interviewer at Interview Better company. Today, I will be assessing your knowledge in the field of Large Language Models (LLMs). \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(redisMock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e883ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationController:\n",
    "    def __init__(self, model_name: str = 'cross-encoder/ms-marco-MiniLM-L-6-v2', threshold: float = -1):\n",
    "        self.reranker = CrossEncoder(model_name)\n",
    "        self.threshold = threshold\n",
    "\n",
    "        self.pending_follow_up: str | None = None\n",
    "        self.missed_in_a_row: int = 0\n",
    "        self.count_followups: int = 0\n",
    "\n",
    "    def reset_conversation(self):\n",
    "        self.missed_in_a_row = 0\n",
    "        self.count_followups = 0\n",
    "        self.pending_follow_up = None\n",
    "\n",
    "    def register_assistant(self, follow_up_q: str):\n",
    "        if follow_up_q == \"DONE\":\n",
    "            self.pending_follow_up = None\n",
    "            self.missed_in_a_row = 0\n",
    "        else:\n",
    "            self.pending_follow_up = follow_up_q\n",
    "        \n",
    "        self.count_followups += 1\n",
    "\n",
    "    def register_user(self, user_ans: str) -> bool:\n",
    "        if self.count_followups >= 5:\n",
    "            return True\n",
    "\n",
    "        if self.pending_follow_up:\n",
    "            if not self._is_related(user_ans, self.pending_follow_up):\n",
    "                self.missed_in_a_row += 1\n",
    "            else:\n",
    "                self.pending_follow_up = None\n",
    "                self.missed_in_a_row = 0\n",
    "\n",
    "        if self.missed_in_a_row >= 2:\n",
    "            self.pending_follow_up = None\n",
    "            self.missed_in_a_row = 0\n",
    "            return True \n",
    "        return False\n",
    "\n",
    "    def _is_related(self, user_ans: str, follow_up_q: str) -> bool:\n",
    "        score = self.reranker.predict([(follow_up_q, user_ans)])[0]\n",
    "        print(user_ans, follow_up_q)\n",
    "        print(score)\n",
    "        print(f\"Re-ranker score: {score:.3f}\")\n",
    "        return score >= self.threshold\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1a23677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# controller = ConversationController()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27abc92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"Do you need a vector store for all text-based LLM use cases?\"\n",
    "# user_answer = \"A\"\n",
    "\n",
    "# controller.register_assistant(question)\n",
    "# controller.register_user(user_answer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41809c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# controller.missed_in_a_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6e30c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_grade_information(graded_result: GradedAnswer):\n",
    "    return f\"\"\"\n",
    "        grade: {graded_result.grade}\n",
    "        explanation of this grade: {graded_result.explanation_of_grade}\n",
    "        follow up question: {graded_result.follow_up_question}\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def conversate_llm(\n",
    "        question: str,\n",
    "        follow_up_question: str | None,\n",
    "        user_answer: str,\n",
    "        ideal_answer: str,\n",
    "        emotion: str,\n",
    "        history_limit: int = 10\n",
    ") -> GradedAnswer:\n",
    "    key_information = (\n",
    "        f\"Original question: {question}\\n\"\n",
    "        f\"Follow up question (if is not None then user answers to this question): {follow_up_question}\\n\"\n",
    "        f\"User answer: {user_answer}\\n\"\n",
    "        f\"Ideal answer: {ideal_answer}\\n\"\n",
    "        f\"User's emotion: {emotion}\\n\"\n",
    "    )\n",
    "    redisMock.add_message(\"user\", key_information)\n",
    "    current_prompt_human = f\"{EVALUATION_INSTRUCTION}\\n\\n{key_information}\"\n",
    "\n",
    "    prompt = build_prompt_with_instruction(\n",
    "        history_obj=redisMock,\n",
    "        current_user_prompt=current_prompt_human,\n",
    "        format_instruction=FINAL_INSTRUCTION,\n",
    "        limit=history_limit,\n",
    "    )\n",
    "\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.1, api_key=OPEN_AI_API_KEY)\n",
    "    chain = prompt | llm | parser_graded\n",
    "    graded: GradedAnswer = chain.invoke({})\n",
    "\n",
    "    redisMock.add_message(\"assistant\", add_grade_information(graded))\n",
    "\n",
    "    return graded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3f371af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalGrade(BaseModel):\n",
    "    grade: int = 0\n",
    "    feedback: str = \"\"\n",
    "\n",
    "\n",
    "parser_final_grade = PydanticOutputParser(pydantic_object=FinalGrade)\n",
    "format_instructions_graded = parser_final_grade.get_format_instructions().replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "\n",
    "SUMMARIZE_INSTRUCTION = f\"\"\"You summarize interview process. \n",
    "You want to give user feedback about the interview process\n",
    "User wants to get feedback about what he should learn to become better at interview process\n",
    "You will be given entire process of interview asking question and user answering it.\n",
    "User's first answer is directly answering to Original question and then he is answering follow up questions.\n",
    "You will have access to grade of assistant and explanations of his grade.\n",
    "In field grade provide final grade,\n",
    "In feedback provide whole feedback about user responses.\n",
    "\n",
    "{format_instructions_graded}\n",
    "\"\"\".strip()\n",
    "\n",
    "def create_final_grade() -> FinalGrade:\n",
    "    messages = redisMock.get_history()\n",
    "    prompt_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"{instruction}\\n\\n{history}\")\n",
    "    ])\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.1, api_key=OPEN_AI_API_KEY)\n",
    "    chain = prompt_template | llm | parser_final_grade\n",
    "    graded: FinalGrade = chain.invoke({\n",
    "        \"instruction\": SUMMARIZE_INSTRUCTION,\n",
    "        \"history\": messages\n",
    "    })\n",
    "    redisMock.clear_messages()\n",
    "\n",
    "    return graded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4c6f707c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"Do you need a vector store for all text-based LLM use cases?\"\n",
    "\n",
    "# user_answer = \"It is used to prevent overfitting in algorithm\"\n",
    "# ideal_answer, cosine_similarity = get_ideal_answer(\"Easy\", \"llm\", question, user_answer)\n",
    "# emotion = \"happy\"\n",
    "\n",
    "# graded_result = conversate_llm(question, None, user_answer, ideal_answer, emotion)\n",
    "\n",
    "# result = chain.invoke({\n",
    "#     \"question\": question,\n",
    "#     \"user_answer\": user_answer,\n",
    "#     \"ideal_answer\": ideal_answer,\n",
    "#     \"emotion\": \"happy\",\n",
    "# })\n",
    "\n",
    "# redisMock.print_last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6269ab8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(redisMock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5f42111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It improves LLMs by specifying them how should it think, so by that model knew what the user specifically wanted How does Chain-of-Thought (CoT) prompting improve reasoning in LLMs?\n",
      "-0.18216534\n",
      "Re-ranker score: -0.182\n",
      "feedback: The user's answer captures the essence of CoT prompting by mentioning that it specifies how the model should think. However, it lacks details about generating intermediate steps, understanding logical flow, and identifying errors in reasoning, which are crucial aspects of the ideal answer.\n",
      "followup question: Can you explain how generating intermediate steps helps in understanding the logical flow of information?\n",
      "If model knew step by step what it should do, so it would do specifically what user wanted Can you explain how generating intermediate steps helps in understanding the logical flow of information?\n",
      "-10.359896\n",
      "Re-ranker score: -10.360\n",
      "Finished asking follow ups\n",
      "This is final grade: 6\n",
      "Interviewer feedback: Your answer provided a basic understanding of how Chain-of-Thought (CoT) prompting improves reasoning in LLMs by indicating that it specifies how the model should think. However, it lacked depth and detail. To enhance your responses in future interviews, consider elaborating on key concepts such as the generation of intermediate steps, the importance of mimicking human reasoning, and how this process aids in identifying errors. Understanding these aspects will help you provide a more comprehensive answer that aligns with the ideal response.\n",
      "--------------------------------------------------\n",
      "For example it could use L2 regularization to pretrained weights. By this model is penalized for deviating from original weights How can LLMs mitigate catastrophic forgetting during fine-tuning?\n",
      "-11.339083\n",
      "Re-ranker score: -11.339\n",
      "feedback: The user provided a relevant answer mentioning L2 regularization, which is a valid technique. However, they did not mention Elastic Weight Consolidation (EWC), which is a more specific approach to mitigate catastrophic forgetting. Therefore, the answer is partially complete.\n",
      "followup question: Can you explain what Elastic Weight Consolidation (EWC) is and how it helps in mitigating catastrophic forgetting?\n",
      "I dont know what this is Can you explain what Elastic Weight Consolidation (EWC) is and how it helps in mitigating catastrophic forgetting?\n",
      "-8.488802\n",
      "Re-ranker score: -8.489\n",
      "Finished asking follow ups\n",
      "This is final grade: 0\n",
      "Interviewer feedback: \n",
      "--------------------------------------------------\n",
      "I dont know How does gradient checkpointing reduce memory usage in training large LLMs?\n",
      "-10.069054\n",
      "Re-ranker score: -10.069\n",
      "feedback: The user answered 'I don't know', which indicates a lack of knowledge on the topic. Therefore, the grade is low. However, the user's happy emotion may suggest a positive attitude towards learning.\n",
      "followup question: DONE\n",
      "Finished asking follow ups\n",
      "This is final grade: 1\n",
      "Interviewer feedback: Your response to the question about gradient checkpointing indicated that you were unsure of the concept, which is a critical topic in training large language models. To improve your interview performance, I recommend studying the principles of gradient checkpointing, including how it helps manage memory usage during training. Understanding the trade-offs involved and being able to explain the process clearly will enhance your confidence and ability to answer similar questions in the future. Additionally, maintaining a positive attitude, as you demonstrated, is beneficial during interviews.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# question = \"Do you need a vector store for all text-based LLM use cases?\"\n",
    "# user_answer = \"For chatbots or conversational agents that rely on predefined rule-based responses, a vector store might not be necessary. It is useful if we want our llm to provide answers based on some specific documents. As an alternative for vector databases we can use decision trees. It is very useful for retrieving information from a large text corpus because we can semantically find best answer for our prompts instead of using basic text NLP algorithms\"\n",
    "# ideal_answer, cosine_similarity = get_ideal_answer(\"Easy\", \"llm\", question, user_answer)\n",
    "# emotion = \"happy\"\n",
    "\n",
    "# graded_result = conversate_llm(question, user_answer, ideal_answer, emotion)\n",
    "\n",
    "# result = chain.invoke({\n",
    "#     \"question\": question,\n",
    "#     \"user_answer\": user_answer,\n",
    "#     \"ideal_answer\": ideal_answer,\n",
    "#     \"emotion\": \"happy\",\n",
    "    \n",
    "# })\n",
    "controller = ConversationController()\n",
    "emotion = \"happy\"\n",
    "\n",
    "for question in questions:\n",
    "    original_question = question\n",
    "    graded_answer = GradedAnswer()\n",
    "    controller.reset_conversation()\n",
    "    controller.register_assistant(question)\n",
    "    user_answer = input(question)\n",
    "    \n",
    "    if user_answer == \"\":\n",
    "        break\n",
    "\n",
    "\n",
    "    if controller.register_user(user_answer):\n",
    "        break\n",
    "\n",
    "    ideal_answer, cosine_similarity = get_ideal_answer(\"Easy\", \"llm\", question, user_answer)\n",
    "    graded_answer = conversate_llm(question, None, user_answer, ideal_answer, emotion)\n",
    "\n",
    "    print(f\"feedback: {graded_answer.explanation_of_grade}\")\n",
    "    print(f\"followup question: {graded_answer.follow_up_question}\")\n",
    "\n",
    "    follow_up_active = True\n",
    "    should_break = False\n",
    "    follow_up_question = graded_answer.follow_up_question\n",
    "\n",
    "    while follow_up_active:\n",
    "        controller.register_assistant(follow_up_question)\n",
    "        user_answer = input(follow_up_question)\n",
    "        \n",
    "        if user_answer == \"\":\n",
    "            should_break = True\n",
    "            break\n",
    "\n",
    "        if controller.register_user(user_answer):\n",
    "            break\n",
    "\n",
    "        graded_answer = conversate_llm(original_question, follow_up_question, user_answer, ideal_answer, emotion)\n",
    "\n",
    "        print(f\"grade: {graded_answer.grade} feedback: {graded_answer.explanation_of_grade}\")\n",
    "        print(f\"followup question: {graded_answer.follow_up_question}\")\n",
    "\n",
    "        if graded_answer.follow_up_question.strip().upper() == \"DONE\":\n",
    "            follow_up_active = False\n",
    "            print(\"-----------------------------\\n\")\n",
    "        else:\n",
    "            follow_up_question = graded_answer.follow_up_question\n",
    "\n",
    "    print(\"Finished asking follow ups\")\n",
    "    finalGrade: FinalGrade = create_final_grade()\n",
    "    print(f\"This is final grade: {finalGrade.grade}\")\n",
    "    print(f\"Interviewer feedback: {finalGrade.feedback}\")\n",
    "    print(50 * \"-\")\n",
    "\n",
    "    if should_break == True:\n",
    "        break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "d3e3612f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: Do you need a vector store for all text-based LLM use cases?\n",
      "answer: For chatbots or conversational agents that rely on predefined rule-based responses, a vector store might not be necessary. It is useful if we want our llm to provide answers based on some specific documents. As an alternative for vector databases we can use decision trees. It is very useful for retrieving information from a large text corpus because we can semantically find best answer for our prompts instead of using basic text NLP algorithms\n",
      "ideal_answer: For chatbots or conversational agents that rely on predefined rule-based responses, a vector store might not be necessary. Instead, these systems often use decision trees or intent recognition to guide responses. Nonetheless, if the chatbot needs to retrieve information from a large text corpus dynamically, integrating a vector store could improve its capabilities.\n",
      "cosine: 1.0\n",
      "grade: 8\n",
      "explanation: The user's answer contains most of the key points from the ideal answer, but it lacks the explicit mention of the improvement in capabilities when integrating a vector store for dynamic information retrieval. The positive emotion in the user's voice also contributed to a higher grade.\n",
      "follow up: What specific improvements can a vector store provide for dynamic information retrieval in chatbots?\n"
     ]
    }
   ],
   "source": [
    "get_data_about_answer(ideal_answer, cosine_similarity, question, user_answer, graded_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "5a68655b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: \n",
      "        Start interview by introducing yourself that you are interviewer in Interview Better company.\n",
      "      \n",
      "assistant: Hello, I am the interviewer from Interview Better company. Today, I will be assessing your knowledge in large language models. \n",
      "user: \n",
      "        Evaluate user's answer on the question from 1 to 10, based on the user answer, ideal answer and cosine similarity calculated between both.\n",
      "        If user's answer does not contain whole information about ideal answer, provide follow-up question to suggest what is missing in the answer.\n",
      "        If user's answer contain whole information provide in follow_up_question field: \"DONE\"\n",
      "        Grade user better if his voice emotion is positive.\n",
      "        Remember to keep the JSON format.\n",
      "    \n",
      "        Question: Do you need a vector store for all text-based LLM use cases?\n",
      "        User answer: For chatbots or conversational agents that rely on predefined rule-based responses, a vector store might not be necessary. It is useful if we want our llm to provide answers based on some specific documents. As an alternative for vector databases we can use decision trees. It is very useful for retrieving information from a large text corpus because we can semantically find best answer for our prompts instead of using basic text NLP algorithms\n",
      "        Ideal answer: For chatbots or conversational agents that rely on predefined rule-based responses, a vector store might not be necessary. Instead, these systems often use decision trees or intent recognition to guide responses. Nonetheless, if the chatbot needs to retrieve information from a large text corpus dynamically, integrating a vector store could improve its capabilities.\n",
      "        User's emotion: happy\n",
      "     \n",
      "assistant: \n",
      "        grade: 8\n",
      "        explanation of this grade: The user's answer contains most of the key points from the ideal answer, but it lacks the explicit mention of the improvement in capabilities when integrating a vector store for dynamic information retrieval. The positive emotion in the user's voice also contributed to a higher grade.\n",
      "        follow up question: What specific improvements can a vector store provide for dynamic information retrieval in chatbots?\n",
      "     \n",
      "user: \n",
      "        Evaluate user's answer on the question from 1 to 10, based on the user answer, ideal answer and cosine similarity calculated between both.\n",
      "        If user's answer does not contain whole information about ideal answer, provide follow-up question to suggest what is missing in the answer.\n",
      "        If user's answer contain whole information provide in follow_up_question field: \"DONE\"\n",
      "        Grade user better if his voice emotion is positive.\n",
      "        Remember to keep the JSON format.\n",
      "    \n",
      "        Question: Do you need a vector store for all text-based LLM use cases?\n",
      "        User answer: For chatbots or conversational agents that rely on predefined rule-based responses, a vector store might not be necessary. It is useful if we want our llm to provide answers based on some specific documents. As an alternative for vector databases we can use decision trees. It is very useful for retrieving information from a large text corpus because we can semantically find best answer for our prompts instead of using basic text NLP algorithms\n",
      "        Ideal answer: For chatbots or conversational agents that rely on predefined rule-based responses, a vector store might not be necessary. Instead, these systems often use decision trees or intent recognition to guide responses. Nonetheless, if the chatbot needs to retrieve information from a large text corpus dynamically, integrating a vector store could improve its capabilities.\n",
      "        User's emotion: happy\n",
      "     \n",
      "assistant: \n",
      "        grade: 8\n",
      "        explanation of this grade: The user's answer contains most of the key points from the ideal answer, but it lacks the explicit mention of the improvement in capabilities when integrating a vector store for dynamic information retrieval. The positive emotion in the user's voice also contributed to a higher grade.\n",
      "        follow up question: What specific improvements can a vector store provide for dynamic information retrieval in chatbots?\n",
      "     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(redisMock)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4219abcd",
   "metadata": {},
   "source": [
    "### 3. Finalize Interview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add2ac24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interview_better",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
