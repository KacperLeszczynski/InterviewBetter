{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04987c18-f798-435c-8729-ce1d60d4ee72",
   "metadata": {},
   "source": [
    "# Audio testing\n",
    "This notebook is focused on finding the best solution on detecting words from audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e24bbc13-9848-41f3-8e47-f44497f42db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import os\n",
    "import time\n",
    "from transformers import pipeline\n",
    "from fuzzywuzzy import process\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "971ade90-0037-4300-a83e-af6005d75558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78fe3817-e129-4cbf-b68a-da68091b0e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPEN_AI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_key = OPEN_AI_API_KEY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa8c262-eed4-4556-a7be-a547b0545263",
   "metadata": {},
   "source": [
    "## 1. Sending audio to backend and then use whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad75b09c-35c5-4e76-bf1c-2ac779472b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = whisper.load_model(\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18eca71f-7a5f-4d65-a306-4cfb49cc222e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(os.path.exists(\"TestAudios/TestAudio1.m4a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f3cace6-c2ce-46a1-9d22-730edf60dbce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.3483726978302"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "result = model.transcribe(\"TestAudios/TestAudio2.m4a\")\n",
    "end = time.time()\n",
    "\n",
    "end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d87829-30ef-421b-8452-d6d8bdb0ee89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79e403c8-122a-402a-aeec-e0076a8c50d5",
   "metadata": {},
   "source": [
    "## 2. WebSpeech API + correcting in backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cc997094-aea7-4f18-9241-340ed01861ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\kacpe\\anaconda3\\envs\\interview_better\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "corrector = pipeline('text2text-generation', model='prithivida/grammar_error_correcter_v1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7fa1512-45cf-48af-881c-bb3d937a9d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text from WebSpeech API \"I think ChromaDB is essential for RAG implementation\n",
    "sample_text = \"I think Roma debate is essential for rock implementation\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "396ad228-da43-4abf-943c-6eca1b5b02c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected = corrector(f\"fix: {sample_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c9425ea-58d0-4327-8c3b-bb493c11e092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think Roma debate is essential for rock implementation.\n"
     ]
    }
   ],
   "source": [
    "print(corrected[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "320956d9-263a-4470-b342-c6b81add8237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Fix: I am using Jake weary and type script in angular Jess project.'}]\n"
     ]
    }
   ],
   "source": [
    "transcript = \"I am using Jake weary and type script in angular Jess project\"\n",
    "corrected = corrector(f\"fix: {transcript}\")\n",
    "\n",
    "print(corrected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd48bd2a-d13e-48cf-ae42-622d47ef34ba",
   "metadata": {},
   "source": [
    "## 2. FuzzyWuzzy + GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a66e3e90-cba5-4ada-9940-3adaddff64bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "TECHNICAL_TERMS = [\n",
    "    \"JSON\", \"jQuery\", \"TypeScript\", \"AngularJS\", \"Vue.js\",\n",
    "    \"C#\", \".NET\", \"GitHub\", \"Stack Overflow\", \"HttpClient\",\n",
    "    \"async\", \"await\", \"REST API\", \"FastAPI\", \"Node.js\",\n",
    "    \"Python\", \"NumPy\", \"Pandas\", \"TensorFlow\", \"PyTorch\"\n",
    "]\n",
    "\n",
    "def fuzzy_correction_phrases(transcript, threshold=80):\n",
    "    corrected_transcript = transcript\n",
    "    for phrase in TECHNICAL_TERMS:\n",
    "        match, score = process.extractOne(phrase, [transcript])\n",
    "        if score >= threshold:\n",
    "            corrected_transcript = corrected_transcript.replace(match, phrase)\n",
    "    return corrected_transcript\n",
    "\n",
    "class LLMCorrector:\n",
    "    def __init__(self):\n",
    "        self.corrector = pipeline(\n",
    "            'text2text-generation',\n",
    "            model='prithivida/grammar_error_correcter_v1',\n",
    "            max_length=128,\n",
    "            device=0 \n",
    "        )\n",
    "\n",
    "    def correct(self, text):\n",
    "        prompt = f\"fix grammar and spelling: {text}\"\n",
    "        corrected = self.corrector(prompt)\n",
    "        return corrected[0]['generated_text']\n",
    "\n",
    "class TechnicalTranscriptCorrector:\n",
    "    def __init__(self):\n",
    "        self.llm_corrector = LLMCorrector()\n",
    "\n",
    "    def correct_transcript(self, transcript):\n",
    "        fuzzy_corrected = fuzzy_correction_phrases(transcript)\n",
    "        fully_corrected = self.llm_corrector.correct(fuzzy_corrected)\n",
    "        return fully_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f936ec0d-489c-49f2-8f67-f23f66ceaff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "corrector = TechnicalTranscriptCorrector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7e3fd8af-ac5d-4f95-9896-2c365ffe160f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW: I wrote a sink function in sea sharp and upload to get help\n",
      "CORRECTED: I wrote a sink function in sea sharp and uploaded to get help.\n"
     ]
    }
   ],
   "source": [
    "raw_transcript = \"I wrote a sink function in sea sharp and upload to get help\"\n",
    "print(\"RAW:\", raw_transcript)\n",
    "\n",
    "corrected_transcript = corrector.correct_transcript(raw_transcript)\n",
    "print(\"CORRECTED:\", corrected_transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af839bd9-ef57-4322-b571-03c258d83925",
   "metadata": {},
   "source": [
    "## 3. GPT 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87b1e31c-d5ec-4b03-b4aa-0a4927eca4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_transcript_with_tech_terms(transcript):\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert technical text corrector.  \n",
    "    Given the input transcript, correct any mistakes related to programming and technical vocabulary.\n",
    "    The input focuses on LLM application, so keep in my mind that vocabulary is directly related to LLMs.\n",
    "\n",
    "    Now correct this transcript:\n",
    "    \"{transcript}\"\n",
    "    \"\"\"\n",
    "\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You correct technical transcripts.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.2\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82adbd27-60af-4bd3-91b1-c9a3e023e8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Modern machine learning applications heavily rely on Python-based frameworks such as PyTorch and TensorFlow.\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5789055824279785"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "raw_transcript = \"modern elements applications heavily rely on python-based Frameworks such as pytorch and Teresa flow utilizing transform architectures like 234 or belt with attention mechanism optimized we are scale dot product calculations I'm fine doing workflows of an integrate techniques like low rack application lower or perimeter efficient fine tuning path within Huggins face Transformers Library production deployments currently utilize Orchestra communication\"\n",
    "corrected = correct_transcript_with_tech_terms(raw_transcript[:len(raw_transcript)//4])\n",
    "end = time.time()\n",
    "print(corrected)\n",
    "end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75e8d971-4f2c-4416-98ad-92733792b5b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"I wrote a `Sink` function in C# and uploaded it to get help.\"'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a5634cf-e682-4a8f-a35b-dae495511ca3",
   "metadata": {},
   "source": [
    "## 4. Local Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b36c2eba-0d19-4201-8393-2ab98aada452",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "MODEL = \"tinyllama\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0079493a-b610-4124-9d35-58430c45573a",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_transcript = \"modern elements applications heavily rely on python-based Frameworks such as pytorch and Teresa flow utilizing transform architectures like 234 or belt with attention mechanism optimized we are scale dot product calculations I'm fine doing workflows of an integrate techniques like low rack application lower or perimeter efficient fine tuning path within Huggins face Transformers Library production deployments currently utilize Orchestra communication\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "aad55e52-51ab-43ff-8373-d5b47db99869",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"\"\"\n",
    "    You are an expert technical text corrector.  \n",
    "    Given the input transcript, correct any mistakes related to programming and technical vocabulary.\n",
    "    The input focuses on LLM application, so keep in my mind that vocabulary is directly related to LLMs.\n",
    "\n",
    "    Now correct this transcript:\n",
    "    \"{raw_transcript}\"\n",
    "    \"\"\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ed07705a-bbb0-4e71-8884-7f8dc59f31b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": False\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "783cb4ad-03cc-40e1-8380-5fe2e71c7e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest â ‹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest â ™ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest â ą \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest â ¸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest â Ľ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling 2af3b81862c6... 100% â–•â–�â–�â–�â–�â–�â–�â–�â–�â–�â–�â–�â–�â–�â–�â–�â–�â–Ź 637 MB                         \u001b[K\n",
      "pulling af0ddbdaaa26... 100% â–•â–�â–�â–�â–�â–�â–�â–�â–�â–�â–�â–�â–�â–�â–�â–�â–�â–Ź   70 B                         \u001b[K\n",
      "pulling c8472cd9daed... 100% â–•â–�â–�â–�â–�â–�â–�â–�â–�â–�â–�â–�â–�â–�â–�â–�â–�â–Ź   31 B                         \u001b[K\n",
      "pulling fa956ab37b8c... 100% â–•â–�â–�â–�â–�â–�â–�â–�â–�â–�â–�â–�â–�â–�â–�â–�â–�â–Ź   98 B                         \u001b[K\n",
      "pulling 6331358be52a... 100% â–•â–�â–�â–�â–�â–�â–�â–�â–�â–�â–�â–�â–�â–�â–�â–�â–�â–Ź  483 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "!ollama pull tinyllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "be6708ae-14c9-4959-b422-0345d9c804a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    \"Please correct any mistakes related to program and technical vocabulary in the following text:\n",
      "\n",
      "      {input_transcript}\n",
      "      Please keep in mind the specific focus on LLM applications, as discussed in the text. Any corrections that are related to programming or technical vocabulary will be relevant for LLMs.\"\n"
     ]
    }
   ],
   "source": [
    "response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)\n",
    "print(response.json()['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2e37f0-e9a0-467b-a693-38efdd54333d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interview_better",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
