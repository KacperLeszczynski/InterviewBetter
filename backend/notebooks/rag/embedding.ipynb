{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "76ab9a4d-e967-4cc5-a416-64d00f22bba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import openai\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "import pandas.io.formats.style\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import chromadb\n",
    "import tiktoken\n",
    "from transformers import AutoTokenizer\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "from langchain_openai import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1f0b16-c7d3-46b7-987f-76311c50d476",
   "metadata": {},
   "source": [
    "### 1. Testing Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35d897d4-c962-495e-a83a-8db16d3f9ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Large language models are transforming natural language processing.\"\n",
    "\n",
    "models = {\n",
    "    \"OpenAI ada-002\": {\n",
    "        \"type\": \"openai\",\n",
    "        \"model\": \"text-embedding-ada-002\"\n",
    "    },\n",
    "    \"MiniLM\": {\n",
    "        \"type\": \"sbert\",\n",
    "        \"model\": \"all-MiniLM-L6-v2\"\n",
    "    },\n",
    "    \"BGE Base\": {\n",
    "        \"type\": \"sbert\",\n",
    "        \"model\": \"BAAI/bge-base-en-v1.5\"\n",
    "    },\n",
    "    # \"E5 Base\": {\n",
    "    #     \"type\": \"sbert\",\n",
    "    #     \"model\": \"intfloat/e5-base\"\n",
    "    # },\n",
    "    # \"E5 Large\": {\n",
    "    #     \"type\": \"sbert\",\n",
    "    #     \"model\": \"intfloat/e5-large\"\n",
    "    # },\n",
    "    \"Snowflake\": {\n",
    "        \"type\": \"sbert\",\n",
    "        \"model\": \"Snowflake/snowflake-arctic-embed-l-v2.0\"\n",
    "    },\n",
    "    # \"Jina\": {\n",
    "    #     \"type\": \"other\",\n",
    "    #     \"model\": \"jinaai/jina-embeddings-v3\"\n",
    "    # },\n",
    "    # \"Solon\": {\n",
    "    #     \"type\": \"solon\",\n",
    "    # }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d5f6381-2623-48cd-b2b6-704b2a6d9bc9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'jinaai' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m jinaai\u001b[38;5;241m/\u001b[39mjina\u001b[38;5;241m-\u001b[39membeddings\u001b[38;5;241m-\u001b[39mv3\n",
      "\u001b[1;31mNameError\u001b[0m: name 'jinaai' is not defined"
     ]
    }
   ],
   "source": [
    "jinaai/jina-embeddings-v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c961e6f6-4343-4559-9ad5-ec72c62b8972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a130db6f-dd8f-47e6-8097-1660cacd6ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPEN_AI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d73c092c-a8ed-4b63-90da-47789291dfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = OPEN_AI_API_KEY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62c7d0c-506d-4398-87f9-c970e9219f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9e49625-02c3-43db-9495-8b1e52aff2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Testowanie modelu: OpenAI ada-002\n",
      "🔍 Testowanie modelu: MiniLM\n",
      "🔍 Testowanie modelu: BGE Base\n",
      "🔍 Testowanie modelu: Snowflake\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_308a5_row0_col2 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #5fba7d 3.2%, transparent 3.2%);\n",
       "}\n",
       "#T_308a5_row1_col2 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #5fba7d 16.0%, transparent 16.0%);\n",
       "}\n",
       "#T_308a5_row2_col2 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #5fba7d 39.5%, transparent 39.5%);\n",
       "}\n",
       "#T_308a5_row3_col2 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #5fba7d 100.0%, transparent 100.0%);\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_308a5\">\n",
       "  <caption>⏱️ Porównanie embeddingów</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_308a5_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_308a5_level0_col1\" class=\"col_heading level0 col1\" >Dimension</th>\n",
       "      <th id=\"T_308a5_level0_col2\" class=\"col_heading level0 col2\" >Time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_308a5_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_308a5_row0_col0\" class=\"data row0 col0\" >MiniLM</td>\n",
       "      <td id=\"T_308a5_row0_col1\" class=\"data row0 col1\" >384</td>\n",
       "      <td id=\"T_308a5_row0_col2\" class=\"data row0 col2\" >0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_308a5_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_308a5_row1_col0\" class=\"data row1 col0\" >BGE Base</td>\n",
       "      <td id=\"T_308a5_row1_col1\" class=\"data row1 col1\" >768</td>\n",
       "      <td id=\"T_308a5_row1_col2\" class=\"data row1 col2\" >0.099000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_308a5_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_308a5_row2_col0\" class=\"data row2 col0\" >Snowflake</td>\n",
       "      <td id=\"T_308a5_row2_col1\" class=\"data row2 col1\" >1024</td>\n",
       "      <td id=\"T_308a5_row2_col2\" class=\"data row2 col2\" >0.244800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_308a5_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_308a5_row3_col0\" class=\"data row3 col0\" >OpenAI ada-002</td>\n",
       "      <td id=\"T_308a5_row3_col1\" class=\"data row3 col1\" >1536</td>\n",
       "      <td id=\"T_308a5_row3_col2\" class=\"data row3 col2\" >0.620400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1e39a9bc5d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbert_models = {}\n",
    "for name, config in models.items():\n",
    "    if config[\"type\"] == \"sbert\":\n",
    "        sbert_models[name] = SentenceTransformer(config[\"model\"])\n",
    "\n",
    "results = []\n",
    "for name, config in models.items():\n",
    "    print(f\"🔍 Testowanie modelu: {name}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if config[\"type\"] == \"openai\":\n",
    "        response = openai.embeddings.create(\n",
    "            input=[text],\n",
    "            model=config[\"model\"]\n",
    "        )\n",
    "        embedding = response.data[0].embedding\n",
    "    elif config[\"type\"] == \"sbert\":\n",
    "        model = sbert_models[name]\n",
    "        if \"e5\" in config[\"model\"]:\n",
    "            text_to_encode = f\"query: {text}\"\n",
    "        else:\n",
    "            text_to_encode = text\n",
    "        embedding = model.encode(text_to_encode, convert_to_numpy=True)\n",
    "    elif config[\"type\"] == \"other\":\n",
    "        model = SentenceTransformer(\"jinaai/jina-embeddings-v3\", trust_remote_code=True)\n",
    "        embedding = model.encode(text_to_encode, convert_to_numpy=True)\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"OrdalieTech/Solon-embeddings-large-0.1\")\n",
    "        model = AutoModel.from_pretrained(\"OrdalieTech/Solon-embeddings-large-0.1\")\n",
    "        embedding = model.encode(text_to_encode, convert_to_numpy=True)\n",
    "\n",
    "\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed = end_time - start_time\n",
    "    dim = len(embedding)\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"Dimension\": dim,\n",
    "        \"Time (s)\": round(elapsed, 4)\n",
    "    })\n",
    "\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df.sort_values(\"Time (s)\", inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.style.bar(subset=[\"Time (s)\"], color=\"#5fba7d\").set_caption(\"⏱️ Porównanie embeddingów\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502118bf-8f36-4788-be0d-fe94d84be212",
   "metadata": {},
   "source": [
    "### 2. Creating Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7302016-ad89-4fab-8621-31edc6c24568",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_CHROMA = \"../../chroma_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec659cf2-59e5-4fc5-a611-f0644b5ea1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.PersistentClient(path=PATH_CHROMA)\n",
    "collection = client.get_or_create_collection(name=\"interview_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36cb2441-403d-4965-ad65-2cd3742685ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = \"Snowflake/snowflake-arctic-embed-l-v2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "89cd0954-0fa5-4ccc-943c-18b733a40de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_model = SentenceTransformer(embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a5cec1e4-8c2d-44e1-8c02-9c23463df446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[     0, 151477,   8484,      2]]), 'attention_mask': tensor([[1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(embedding_model)\n",
    "tokens = tokenizer(\"Twój tekst\", return_tensors=\"pt\")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1034cc88-9732-4e58-ad40-38e82b007587",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "91e952cd-f63d-482c-aa98-bf4c6b4049b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = \"https://skphd.medium.com/large-language-model-llm-interview-questions-ded6264547f1\"\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f12ca158-3d0f-4429-a270-6f7526c86946",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "pattern = re.compile(r\"^\\d+\\.\\s?.+\")\n",
    "\n",
    "questions = []\n",
    "for strong in soup.find_all(\"strong\"):\n",
    "    text = strong.get_text(strip=True)\n",
    "    if pattern.match(text):\n",
    "        questions.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "092a8cbc-53ca-4d60-8369-7d5d6aad3a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_python = \"https://www.datainterview.com/blog/top-100-python-interview-questions\"\n",
    "response_python = requests.get(url_python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "129cedbd-d340-4bec-8262-f1f8769685f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Do you need a vector store for all text-based LLM use cases?\n"
     ]
    }
   ],
   "source": [
    "print(questions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "c3f2645f-d6d8-4b64-9ca4-cf76d1e89dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_questions(h3_class):\n",
    "    h3 = soup.find('h3', class_=h3_class)\n",
    "    print(h3)\n",
    "    if h3:\n",
    "        next_ol = h3.find_next_sibling(['ol', 'ul'])  # szukamy tylko list\n",
    "        if next_ol:\n",
    "            return [li.get_text(strip=True) for li in next_ol.find_all('li')]\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "6f49027e-ad2c-4343-822d-cea188492ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "basic_questions = extract_questions('basic-python-interview-questions')\n",
    "intermediate_questions = extract_questions('intermediate-python-interview-questions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "006e05e5-71ef-4f1c-8e06-f3189c9ab954",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_questions = [\n",
    "    \"How do you remove duplicates from a list while maintaining order?\",\n",
    "    \"What is the purpose of the enumerate() function, and how do you use it?\",\n",
    "    \"Explain how list slicing works, including the use of negative indices.\",\n",
    "    \"How do you flatten a nested list in Python?\",\n",
    "    \"What is the difference between append() and extend() in lists?\",\n",
    "    \"How do you use the zip() function in Python?\",\n",
    "    \"How do you find the frequency of elements in a list?\",\n",
    "    \"What are Python comprehensions, and how do they improve code readability?\",\n",
    "    \"Explain the difference between isinstance() and type checking with type().\",\n",
    "    \"How do you merge multiple dictionaries in Python 3.9+?\",\n",
    "    \"What are Python closures, and how do they work?\",\n",
    "    \"How do you reverse the order of words in a string?\",\n",
    "    \"What is the purpose of collections.defaultdict()?\",\n",
    "    \"How do you check if all elements in a list are unique?\",\n",
    "    \"What is the purpose of collections.OrderedDict()?\",\n",
    "    \"Explain how to create a recursive function in Python.\",\n",
    "    \"What are lambda functions, and when are they most useful?\",\n",
    "    \"How do you create a simple iterator class in Python?\",\n",
    "    \"What is the itertools.product() function, and where is it useful?\",\n",
    "    \"How do you handle large files in Python without loading the entire file into memory?\",\n",
    "    \"How do you implement a binary search algorithm in Python?\",\n",
    "    \"Explain the concept and use of decorators in Python.\",\n",
    "    \"How do you implement a custom sorting function using sorted() with a lambda?\",\n",
    "    \"What is the functools.reduce() function, and how does it work?\",\n",
    "    \"How do you implement memoization in Python?\",\n",
    "    \"How do you use the collections.Counter() for counting elements in an iterable?\",\n",
    "    \"Implement a function that generates all permutations of a given string.\",\n",
    "    \"What is the use of heapq, and how do you create a min-heap?\",\n",
    "    \"Explain the with statement and how it manages resources.\",\n",
    "    \"How do you implement a queue using Python's deque?\",\n",
    "    \"What is multithreading, and how does Python handle it with the GIL?\",\n",
    "    \"Explain how to build a context manager using contextlib.\",\n",
    "    \"How do you work with JSON data in Python?\",\n",
    "    \"Write a function that merges overlapping intervals in a list of tuples.\",\n",
    "    \"How do you parallelize code execution using Python's multiprocessing module?\",\n",
    "    \"How do you handle CSV data with large file sizes using Pandas efficiently?\",\n",
    "    \"Explain the concept of method chaining in Python.\",\n",
    "    \"How do you use the pathlib module for file system operations?\",\n",
    "    \"What is asyncio, and how do you use it for asynchronous programming?\",\n",
    "    \"How do you use pandas.groupby() to perform complex data aggregation?\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4281d854-8879-4732-8085-b9c48d8dc065",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabeledAnswer(BaseModel):\n",
    "    explanation: str\n",
    "\n",
    "class Answers(BaseModel):\n",
    "    answers: List[LabeledAnswer]\n",
    "    difficulty: str\n",
    "\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Answers)\n",
    "\n",
    "format_instructions = parser.get_format_instructions().replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that answers questions with explanations and difficulty ratings.\"),\n",
    "    (\"human\", f\"\"\"Please answer the following question by generating **10 different correct answers**. 3 of them should be in 1-2 sentences, 4 of them should be in 3-4, the other 3 should be in 4-5 sentences.\n",
    "For each answer, provide:\n",
    "- explanation\n",
    "for question provide:\n",
    "- difficulty level (Easy, Medium, Hard, Very Hard)\n",
    "\n",
    "Return the result strictly in this JSON format:\n",
    "{format_instructions}\n",
    "\n",
    "Question: {{question}}\"\"\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "cbe2a3c2-668d-409c-b314-2d755180a62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.7, api_key=OPEN_AI_API_KEY) \n",
    "\n",
    "chain = prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "590444cf-2754-4dae-b155-0371e795c7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.invoke({\n",
    "    \"question\": \"What is overfitting in machine learning?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8f9a454b-8457-4d28-9006-c7fc2ab8d746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledAnswer(explanation='Overfitting occurs when a machine learning model learns the training data too well, capturing noise and outliers as if they were true patterns.'), LabeledAnswer(explanation=\"Overfitting happens when a model's complexity is too high relative to the amount of data available, leading it to perform poorly on unseen data.\"), LabeledAnswer(explanation='Overfitting is a modeling error that occurs when a model is too complex and fits the training data too closely.'), LabeledAnswer(explanation='In machine learning, overfitting is a condition where a model performs well on training data but poorly on new, unseen data. This often arises when the model captures noise and random fluctuations in the training set instead of general patterns. To prevent overfitting, techniques such as cross-validation, regularization, and simplifying the model are often employed.'), LabeledAnswer(explanation='Overfitting is a common pitfall in machine learning where a model becomes too tailored to the specific dataset it was trained on. It captures not only the underlying trends but also the noise within the training data, leading to a loss of generalization. This means while the model has low error on the training data, it has high error when applied to new data. Techniques like pruning, regularization, or using more data can mitigate overfitting.'), LabeledAnswer(explanation=\"When a machine learning model overfits, it indicates that the model has learned the details and noise in the training data to an extent that it negatively impacts the model's performance on new data. This happens because the model becomes too complex, picking up on random fluctuations rather than the intended outputs. As a result, while the model may show impressive performance metrics on the training dataset, it fails to generalize to other datasets.\"), LabeledAnswer(explanation='Overfitting is a situation where a machine learning model learns not only the true underlying patterns but also the noise in the training data. This results in a model that performs exceptionally on the data it was trained on, but poorly on new, unseen data. Overfitting is more likely to occur with models that are too complex relative to the amount of data available. To combat overfitting, techniques such as reducing model complexity, using more data, and applying regularization methods can be used.'), LabeledAnswer(explanation='Overfitting in machine learning refers to a model that fits the training data too closely and captures noise as if it were an actual signal. This often results in a model that performs well on training data but fails to generalize to new, unseen datasets. As the model complexity increases, the risk of overfitting also rises. Regularization techniques, such as L1 and L2 regularization, can help control overfitting by adding a penalty to the loss function.'), LabeledAnswer(explanation='Overfitting is a modeling error that arises when a machine learning model learns the detail and noise in the training data to the extent it negatively impacts the performance of the model on new data. It occurs when the model is too complex, such as having too many parameters relative to the amount of training data. This results in the model being tuned to the training data rather than learning the general trends that apply to new data. The primary goal in machine learning is to have a model that generalizes well to new data, hence avoiding overfitting is crucial.'), LabeledAnswer(explanation='Overfitting is a scenario in which a machine learning model captures the training data too well, including its noise and random fluctuations. This leads to a model that may have very high accuracy on the training data but performs poorly on unseen data. It is often a result of having too many features or parameters in the model, making it overly complex. To address overfitting, strategies such as using simpler models, increasing the size of the training dataset, or employing techniques like dropout in neural networks are commonly used.')]\n"
     ]
    }
   ],
   "source": [
    "print(result.answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c54d8d55-cbeb-4412-9fb0-548905f71c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01234567891011121314151617181920212223242526272829303132333435363738394041424344454647"
     ]
    }
   ],
   "source": [
    "answers_list = []\n",
    "\n",
    "for i, question in enumerate(questions):\n",
    "    result = chain.invoke({\n",
    "        \"question\": question\n",
    "    })\n",
    "\n",
    "    answers_list.append(result)\n",
    "\n",
    "    print(i, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0f7aef22-79c4-4b35-9651-ee0bfcfb5c75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"When fine-tuning LLMs, ensuring that the model retains knowledge from previously learned tasks is crucial. Catastrophic forgetting occurs when the model's neural network overwrites old information with new information, leading to a loss of previously acquired skills or knowledge. This can be mitigated by techniques such as rehearsal, regularization, or using a multi-task learning framework to maintain a balance between old and new data.\""
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers_list[4].answers[4].explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "348838b1-ba37-40e9-a68b-6c65d541bc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaDatas(BaseModel):\n",
    "    difficulty: str\n",
    "    type_question: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d1238d-2a7d-45ca-959a-912e13ace0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "    collection.add(\n",
    "        ids=[doc_id],\n",
    "        embeddings=[embedding],\n",
    "        metadatas=[{\"source\": \"stackoverflow\", \"question\": doc[\"question\"], \"url\": doc[\"url\"], \"version\": doc[\"version\"]}],\n",
    "        documents=[chunk]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "877363cc-27e0-4a54-b7c8-0cb02ff90a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_chromadb(ID, type_question, answers_list):\n",
    "    en_id = 0\n",
    "    \n",
    "    for answer_list in answers_list:\n",
    "        difficulty = answer_list.difficulty\n",
    "        metadatas = MetaDatas(difficulty=difficulty, type_question=type_question)\n",
    "        \n",
    "        for idx, answer in enumerate(answer_list.answers):\n",
    "            explanation = answer.explanation\n",
    "            exp_id = f\"{ID}{en_id}_{idx}\"        \n",
    "            embedding = sentence_model.encode(explanation, convert_to_numpy=True)\n",
    "            \n",
    "            \n",
    "            collection.add(\n",
    "                ids=[exp_id],\n",
    "                embeddings=[embedding],\n",
    "                metadatas=[{\"difficulty\": metadatas.difficulty, \"type_question\": metadatas.type_question}],\n",
    "                documents=[explanation]\n",
    "            )\n",
    "    \n",
    "        en_id += 1\n",
    "        print(en_id, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "34c7ded5-2120-4767-a97f-0ae40d10426a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123456789101112131415161718192021222324252627282930313233343536373839404142434445464748"
     ]
    }
   ],
   "source": [
    "add_to_chromadb(\"LLM_QUESTION_\", \"llm\", answers_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b40624c7-64d6-47f5-b625-515c56390fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0123456789101112131415161718192021222324252627282930313233343536373839"
     ]
    }
   ],
   "source": [
    "answers_python_list = []\n",
    "\n",
    "for i, question in enumerate(python_questions):\n",
    "    result = chain.invoke({\n",
    "        \"question\": question\n",
    "    })\n",
    "\n",
    "    answers_python_list.append(result)\n",
    "\n",
    "    print(i, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "f32a7e01-033a-4f1a-87aa-bfa4bf0276b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12345678910111213141516171819202122232425262728293031323334353637383940"
     ]
    }
   ],
   "source": [
    "add_to_chromadb(\"PYTHON_QUESTION_\", \"python\", answers_python_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "2c375848-703b-4c2b-9bf6-159716bdd8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answers=[LabeledAnswer(explanation='The append() method adds a single element to the end of the list, while extend() adds all elements of an iterable to the list.'), LabeledAnswer(explanation='extend() will iterate over its argument adding each element to the list, while append() treats its argument as a single object and adds it to the list.'), LabeledAnswer(explanation='The append() function modifies the list by adding its argument as one element, whereas extend() takes an iterable and adds each of its elements to the list.'), LabeledAnswer(explanation='When you use append(), you are adding exactly one new element to the list, which could be any object, including another list. In contrast, extend() requires an iterable and adds each of its elements to the existing list. This means that appending a list will result in a list within a list, while extending with a list will combine the elements.'), LabeledAnswer(explanation='append() is typically used when you want to add a single item to the end of a list, regardless of its type. In contrast, extend() is used when you want to concatenate the existing list with another iterable, like a list, tuple, or string. The key difference is that append() adds its argument as a whole, while extend() unpacks the iterable and adds each element individually.'), LabeledAnswer(explanation='append() is a method that adds its argument as a single element at the end of a list, which can be any Python object. On the other hand, extend() takes an iterable and appends each of its elements to the list individually. This means using append() with a list will result in a nested list, while extend() will merge the elements of the iterable into the original list.'), LabeledAnswer(explanation='Using append() will add a single object to the end of a list, treating the entire object as one element. In contrast, extend() takes each element from an iterable and adds it to the list, effectively expanding the list by the number of elements in the iterable. This means append() is suitable for adding one item, while extend() is ideal for adding multiple items from another iterable.'), LabeledAnswer(explanation=\"The append() method is used to add a single new item to the end of a list, and it doesn’t matter what type of object you append, it will always be treated as one element. On the other hand, extend() is used to add multiple elements from an iterable to the list. When extend() is called with an iterable, it breaks the iterable into individual elements and appends each one to the list, effectively 'extending' the list with those elements.\"), LabeledAnswer(explanation=\"The main difference between append() and extend() lies in how they handle their input. append() takes a single argument and adds that object to the end of the list. This means if you append a list, it becomes a single element of the outer list, resulting in a nested structure. Conversely, extend() accepts an iterable as its argument and iterates over it, adding each element to the list. This effectively concatenates the iterable's items to the list, providing a way to combine lists or add multiple elements at once.\"), LabeledAnswer(explanation='append() and extend() are both list methods that modify the list in place, but they do so in different ways. append() is used to add a single object to the end of the list. For example, appending a list to another list will result in a list of lists. extend(), however, is designed to take an iterable and add each of its elements to the list, effectively merging the iterable into the list. This makes extend() more suitable for concatenating lists or adding multiple values at once, as it adds each element of the iterable separately to the list.')] difficulty='Medium'\n"
     ]
    }
   ],
   "source": [
    "print(answers_python_list[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "ddd98e0c-216b-463b-8c23-6a29a8e88601",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = collection.query(\n",
    "    query_embeddings=[sentence_model.encode(\"What is append in python?\", convert_to_numpy=True)],\n",
    "    n_results=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "e1eb7a3a-eed3-485c-b020-8301dd67e5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': [['PYTHON_QUESTION_4_5', 'PYTHON_QUESTION_4_2', 'PYTHON_QUESTION_4_0']], 'embeddings': None, 'documents': [['append() is a method that adds its argument as a single element at the end of a list, which can be any Python object. On the other hand, extend() takes an iterable and appends each of its elements to the list individually. This means using append() with a list will result in a nested list, while extend() will merge the elements of the iterable into the original list.', 'The append() function modifies the list by adding its argument as one element, whereas extend() takes an iterable and adds each of its elements to the list.', 'The append() method adds a single element to the end of the list, while extend() adds all elements of an iterable to the list.']], 'uris': None, 'included': ['metadatas', 'documents', 'distances'], 'data': None, 'metadatas': [[{'difficulty': 'Medium', 'type_question': 'python'}, {'type_question': 'python', 'difficulty': 'Medium'}, {'type_question': 'python', 'difficulty': 'Medium'}]], 'distances': [[0.5214828252792358, 0.6733735799789429, 0.6860897541046143]]}\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
