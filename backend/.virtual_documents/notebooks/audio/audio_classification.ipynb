import os
from sklearn.preprocessing import LabelEncoder, StandardScaler
import librosa
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score
from collections import Counter
from xgboost import XGBClassifier
import matplotlib.pyplot as plt
from audiomentations import Compose, AddGaussianNoise, Gain, Shift, ClippingDistortion, PitchShift
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D, LeakyReLU
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
import seaborn as sns
from tensorflow.keras.regularizers import l2
from sklearn.utils.class_weight import compute_class_weight
import cv2
from tensorflow.keras.applications import VGG16
import tensorflow as tf
from tensorflow import keras


print("TensorFlow version:", tf.__version__)
print("GPU dostępne:", tf.config.list_physical_devices('GPU'))





root_folder = './RavDess'





def create_X_y():
    X_filepath = []
    y_class = []
    
    emotion_map = {
        '01': 'neutral',
        '02': 'calm',
        '03': 'happy',
        '04': 'sad',
        '05': 'angry',
        '06': 'fearful',
        '07': 'disgust',
        '08': 'surprised'
    }
    
    
    for dirpath, dirnames, filenames in os.walk(root_folder):
        for filename in filenames:
            filepath = os.path.join(dirpath, filename)
            y = filename.split("-")[2]
            emotion = emotion_map[y]
    
            if emotion:
                X_filepath.append(filepath)
                y_class.append(emotion)

    return X_filepath, y_class


X_filepath = []
y_class = []

emotion_map = {
    '01': 'neutral',
    '02': 'calm',
    '03': 'happy',
    '04': 'sad',
    '05': 'angry',
    '06': 'fearful',
    '07': 'disgust',
    '08': 'surprised'
}


for dirpath, dirnames, filenames in os.walk(root_folder):
    for filename in filenames:
        filepath = os.path.join(dirpath, filename)
        y = filename.split("-")[2]
        emotion = emotion_map[y]

        if emotion:
            X_filepath.append(filepath)
            y_class.append(emotion)


print(X_filepath[0], y_class[0])


len(y_class)


len(X_filepath)


labelEncoder = LabelEncoder()

y_mapped = labelEncoder.fit_transform(y_class)


class_counts = Counter(y_mapped)
print(class_counts)


def augment_audio(y, sr=16000):
    augmented = []

    # Dodaj lekki szum
    noise = y + 0.002 * np.random.randn(len(y))
    augmented.append(noise)

    # Pitch shift +1 semiton
    pitch_up = librosa.effects.pitch_shift(y, sr=sr, n_steps=1)
    augmented.append(pitch_up)

    # Time stretch (zwolnienie)
    try:
        stretch = librosa.effects.time_stretch(y, rate=0.9)
        # Docięcie lub padding, żeby długość się zgadzała
        if len(stretch) > len(y):
            stretch = stretch[:len(y)]
        else:
            stretch = np.pad(stretch, (0, len(y) - len(stretch)))
        augmented.append(stretch)
    except:
        pass  # czasem librosa może rzucić błąd jeśli tempo niestabilne

    return augmented


def test_model(model, X_train, X_test, y_train, y_test):
    y_pred = model.predict(X_test)
    y_pred_train = model.predict(X_train)
    
    print("accuracy of model: ", accuracy_score(y_test, y_pred))
    print("accuracy of model (train data): ", accuracy_score(y_train, y_pred_train))





def load_audio_fixed_length(path, sr=16000, duration=3.0):
    samples = int(sr * duration)
    y, _ = librosa.load(path, sr=sr, mono=True)
    if len(y) > samples:
        y = y[:samples]
    else:
        y = np.pad(y, (0, samples - len(y)))
    return y


def extract_mfcc_features(y, sr=16000, n_mfcc=13):
    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)
    mfcc_delta = librosa.feature.delta(mfcc)
    mfcc_delta2 = librosa.feature.delta(mfcc, order=2)

    def summarize(feat):
        return np.concatenate([np.mean(feat, axis=1), np.std(feat, axis=1)])

    features = np.concatenate([
        summarize(mfcc),
        summarize(mfcc_delta),
    ])

    return features


X_mfcc = []

for path in X_filepath:
    y = load_audio_fixed_length(path)
    features = extract_mfcc_features(y)
    X_mfcc.append(features)

X_mfcc = np.array(X_mfcc)






print(X_filepath[0])
labelEncoder.inverse_transform(y_mapped)


X_train_filepath, X_test_filepath, y_train, y_test = train_test_split(X_filepath, y_mapped, random_state=42, train_size=0.8)


print(X_train_filepath[0], y_train[0])


X_train_augmented = []
y_train_augmented = []

for path, label in zip(X_train_filepath, y_train):
    y = load_audio_fixed_length(path)
    base_feat = extract_mfcc_features(y)
    
    X_train_augmented.append(base_feat)
    y_train_augmented.append(label)

    for aug_y in augment_audio(y):
        aug_feat = extract_mfcc_features(aug_y)
        X_train_augmented.append(aug_feat)
        y_train_augmented.append(label)

X_train_augmented = np.array(X_train_augmented)
y_train_augmented = np.array(y_train_augmented)


X_test = []
for path in X_test_filepath:
    y = load_audio_fixed_length(path)
    features = extract_mfcc_features(y)
    X_test.append(features)
X_test = np.array(X_test)





scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_augmented)
X_test_scaled = scaler.transform(X_test)


# X_test_scaled[0]


len(X_train)


len(X_test)


svm = SVC(kernel="rbf", C=10, gamma='scale')
svm.fit(X_train_scaled, y_train_augmented)


test_model(svm, X_train_scaled, X_test_scaled, y_train_augmented, y_test)








X_train, X_test, y_train, y_test = train_test_split(X_mfcc, y_mapped, random_state=42, train_size=0.8)


rf = RandomForestClassifier(
    n_estimators=500,       
    max_depth=6,        
    random_state=42
)
rf.fit(X_train_augmented, y_train_augmented)


test_model(rf, X_train_augmented, X_test, y_train_augmented, y_test)








xgb = XGBClassifier(
    n_estimators=200,
    max_depth=5,
    learning_rate=0.2,
    use_label_encoder=False,
    eval_metric='mlogloss',
    random_state=42
)
xgb.fit(X_train_augmented, y_train_augmented)


test_model(xgb, X_train_augmented, X_test, y_train_augmented, y_test)


param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.05, 0.1, 0.2]
}

grid = GridSearchCV(XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'),
                    param_grid, cv=3, verbose=2)
grid.fit(X_train_augmented, y_train_augmented)

print("Best params:", grid.best_params_)
print("Best score:", grid.best_score_)


test_model(xgb, X_train_augmented, X_test, y_train_augmented, y_test)





def get_mel_spectrogram(file_path, sr=16000, n_mels=128, duration=4, apply_aug=False, augmenter=None):
    y, sr = librosa.load(file_path, sr=sr, duration=duration)
    
    if len(y) < sr * duration:
        y = np.pad(y, (0, sr * duration - len(y)))
    
    if apply_aug and augmenter:
        y = augmenter(samples=y, sample_rate=sr)

    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels, )
    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)
    
    if mel_spec_db.shape[1] < 128:
        mel_spec_db = np.pad(mel_spec_db, ((0, 0), (0, 128 - mel_spec_db.shape[1])))
    elif mel_spec_db.shape[1] > 128:
        mel_spec_db = mel_spec_db[:, :128]
    
    return mel_spec_db

def show_spectrogram(filepath):
    spec = get_mel_spectrogram(filepath)
    plt.imshow(spec, aspect='auto', origin='lower')
    plt.title("Mel-Spectrogram")
    plt.colorbar()
    plt.show()



show_spectrogram("./RavDess\\Actor_02\\03-01-01-01-01-01-02.wav")


X_filepath, y = create_X_y()


X_filepath[0], y[0]


y[0]


augmenter = Compose([
    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),
    Shift(min_shift=-0.1, max_shift=0.1, p=0.5),
    Gain(min_gain_db=-3, max_gain_db=3, p=0.5),
])


def get_augmented_data(augmenter, X_filepath, y, how_many):
    X = []
    Y = []
    
    for path, label in zip(X_filepath, y):
        X.append(get_mel_spectrogram(path))
        Y.append(label)
        
        for _ in range(how_many):
            X.append(get_mel_spectrogram(path, apply_aug=True, augmenter=augmenter))
            Y.append(label)

    return X, Y


X = []
Y = []

for path, label in zip(X_filepath, y):
    X.append(get_mel_spectrogram(path))
    Y.append(label)
    
    for _ in range(2):
        X.append(get_mel_spectrogram(path, apply_aug=True, augmenter=augmenter))
        Y.append(label)


le = LabelEncoder()
y_encoded = le.fit_transform(Y)


print(Counter(y_encoded))


X = np.array(X)
X = X[..., np.newaxis]



X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42)



model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 1)),
    BatchNormalization(),
    MaxPooling2D((2, 2)),

    Conv2D(64, (3, 3), activation='relu'),
    BatchNormalization(),
    MaxPooling2D((2, 2)),

    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.3),
    Dense(len(np.unique(y_encoded)), activation='softmax')
])


model.compile(optimizer=Adam(learning_rate=0.001),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])


model.fit(X_train, y_train,
          validation_data=(X_val, y_val),
          epochs=30,
          batch_size=32)


loss, acc = model.evaluate(X_val, y_val)
print(f"Validation accuracy: {acc:.4f}")


y_pred_probs = model.predict(X_val)
y_pred = np.argmax(y_pred_probs, axis=1)

# Confusion matrix
cm = confusion_matrix(y_val, y_pred)
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=le.classes_, yticklabels=le.classes_)
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix")
plt.show()

# Classification report
print(classification_report(y_val, y_pred, target_names=le.classes_))





model_v2 = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 1)),
    BatchNormalization(),
    MaxPooling2D((2, 2)),

    Conv2D(64, (3, 3), activation='relu'),
    BatchNormalization(),
    MaxPooling2D((2, 2)),

    Conv2D(128, (3, 3), activation='relu'),
    BatchNormalization(),
    MaxPooling2D((2, 2)),

    GlobalAveragePooling2D(),

    Dense(256, activation='relu'),
    Dropout(0.4),
    Dense(len(le.classes_), activation='softmax')
])


model_v2.compile(
    optimizer=Adam(learning_rate=0.001),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

model_v2.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=50,
    batch_size=32,
    callbacks=[early_stopping]
)


loss, acc = model_v2.evaluate(X_val, y_val)
print(f"Validation accuracy: {acc:.4f}")


model_v2.save('model_v2.keras')


X_2, Y_2 = get_augmented_data(augmenter_new, X_filepath, y, 4)



X_2 =  np.array(X_2)
X_2 = X_2[..., np.newaxis]


len(X_2)


X_2 = (X_2 - X_2.mean()) / X_2.std()


le_new = LabelEncoder()
y_encoded = le_new.fit_transform(Y_2)


np.save("classes.npy", le_new.classes_)



X_train, X_val, y_train, y_val = train_test_split(X_2, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42)



model = keras.models.load_model("model_v3.keras")





spectrogram = model.predict(X_2[0][np.newaxis, ...])


spectrogram


predicted_index = int(np.argmax(spectrogram))         # np. 5
confidence = float(np.max(spectrogram))               # np. 0.893


print(predicted_index, confidence)


predicted_label = le_new.inverse_transform([predicted_index])[0]



print(predicted_label)


len(X_train)


X_train[0]


Counter(y_encoded)





augmenter_new = Compose([
    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),
    Shift(min_shift=-0.1, max_shift=0.1, p=0.5),
    Gain(min_gain_db=-3, max_gain_db=3, p=0.5),
    ClippingDistortion(min_percentile_threshold=0, max_percentile_threshold=10, p=0.3),
    PitchShift(min_semitones=-1, max_semitones=1, p=0.5),
])


model_v3 = Sequential([
    Conv2D(32, (5, 5), activation='relu', input_shape=(128, 128, 1)),
    BatchNormalization(),
    LeakyReLU(alpha=0.1),
    MaxPooling2D((2, 2)),
    Dropout(0.2),

    Conv2D(64, (3, 3), kernel_regularizer=l2(0.001)),
    BatchNormalization(),
    LeakyReLU(alpha=0.1),
    MaxPooling2D((2, 2)),
    Dropout(0.2),

    Conv2D(128, (3, 3), kernel_regularizer=l2(0.001)),
    BatchNormalization(),
    LeakyReLU(alpha=0.1),
    MaxPooling2D((2, 2)),
    Dropout(0.2),

    Conv2D(256, (3, 3), kernel_regularizer=l2(0.001)),
    BatchNormalization(),
    LeakyReLU(alpha=0.1),
    MaxPooling2D((2, 2)),
    Dropout(0.2),

    GlobalAveragePooling2D(),

    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(len(le_new.classes_), activation='softmax')
])


class_weights = compute_class_weight('balanced', classes=np.unique(y_encoded), y=y_encoded)
class_weights = dict(enumerate(class_weights))

model_v3.compile(
    optimizer=Adam(learning_rate=0.003),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)

history = model_v3.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=130,
    batch_size=32,
    callbacks=[early_stopping, reduce_lr],
    class_weight=class_weights
)



loss, acc = model_v3.evaluate(X_val, y_val)
print(f"Validation accuracy: {acc:.4f}")


model_v3.save("model_v4.keras")


model_v3.save_weights("model_v4.weights.h5")





def get_mel_spectrogram_vgg(file_path, sr=16000, n_mels=128, duration=4, apply_aug=False, augmenter=None):
    y, sr = librosa.load(file_path, sr=sr, duration=duration)
    
    if len(y) < sr * duration:
        y = np.pad(y, (0, sr * duration - len(y)))
    
    if apply_aug and augmenter:
        y = augmenter(samples=y, sample_rate=sr)

    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)
    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)

    # Normalizacja do 0–255 jako obraz
    mel_spec_norm = cv2.normalize(mel_spec_db, None, 0, 255, cv2.NORM_MINMAX)
    mel_spec_norm = mel_spec_norm.astype(np.uint8)

    # Resize do 224x224
    mel_resized = cv2.resize(mel_spec_norm, (224, 224))

    # Zamiana na RGB (z grayscale)
    mel_rgb = cv2.cvtColor(mel_resized, cv2.COLOR_GRAY2RGB)

    return mel_rgb.astype(np.float32) / 255.0  # skala 0–1, shape (224, 224, 3)

def get_augmented_data(augmenter, X_filepath, y, how_many):
    X = []
    Y = []
    
    for path, label in zip(X_filepath, y):
        X.append(get_mel_spectrogram_vgg(path))
        Y.append(label)
        
        for _ in range(how_many):
            X.append(get_mel_spectrogram_vgg(path, apply_aug=True, augmenter=augmenter))
            Y.append(label)

    return X, Y


augmenter_new_transfer = Compose([
    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),
    Shift(min_shift=-0.1, max_shift=0.1, p=0.5),
    Gain(min_gain_db=-3, max_gain_db=3, p=0.5),
    ClippingDistortion(min_percentile_threshold=0, max_percentile_threshold=10, p=0.3),
])


X_filepath, y = create_X_y()


X_2, Y_2 = get_augmented_data(augmenter_new_transfer, X_filepath, y, 3)


X_2[0].shape






le_new = LabelEncoder()
y_encoded = le_new.fit_transform(Y_2)


X_train, X_val, y_train, y_val = train_test_split(X_2, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42)



X_train = np.array(X_train, dtype=np.float32)
X_val = np.array(X_val, dtype=np.float32)


print(len(X_train), len(y_train), len(X_val), len(y_val))


base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
base_model.trainable = False 

x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dropout(0.3)(x)
x = Dense(128, activation='relu')(x)
x = Dropout(0.3)(x)
output = Dense(len(np.unique(y_encoded)), activation='softmax')(x)

model = Model(inputs=base_model.input, outputs=output)
model.compile(optimizer=Adam(learning_rate=1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])



early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)


history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=15,
    batch_size=32,
    callbacks=[early_stopping, reduce_lr],

)


model.save("model_vgg.keras")


model.save_weights("model_vgg.weights.h5")





model = keras.models.load_model('model_v4.keras')



